<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yeliang Xiu&#39;Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xiuyeliang.com/"/>
  <updated>2020-12-10T13:16:59.742Z</updated>
  <id>http://xiuyeliang.com/</id>
  
  <author>
    <name>Yeliang Xiu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>The Review of commonsense-based QA Benchmarks in Recent Years</title>
    <link href="http://xiuyeliang.com/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/"/>
    <id>http://xiuyeliang.com/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/</id>
    <published>2020-12-10T12:46:29.000Z</published>
    <updated>2020-12-10T13:16:59.742Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-QuAIL-Getting-Closer-to-AI-Complete-Question-Answering-A-Set-of-Prerequisite-Real-Tasks-AAAI2020"><a href="#1-QuAIL-Getting-Closer-to-AI-Complete-Question-Answering-A-Set-of-Prerequisite-Real-Tasks-AAAI2020" class="headerlink" title="1. QuAIL: Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks, AAAI2020"></a>1. QuAIL: Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks, AAAI2020</h2><h3 id="1-1-Author"><a href="#1-1-Author" class="headerlink" title="1.1 Author"></a>1.1 Author</h3><p>Anna Rogers,Anna Rumshisky–University of Massachusetts Lowell</p><h3 id="1-2-Description"><a href="#1-2-Description" class="headerlink" title="1.2 Description"></a>1.2 Description</h3><p>QuAIL, the ﬁrst RC dataset to combine text-based, world knowledge and unanswerable questions, and to provide question type annotation that would enable diagnostics of the reasoning strategies by a given QA system. QuAIL contains 15K multi-choice questions for 800 texts in 4 domains.</p><h3 id="1-3-Example"><a href="#1-3-Example" class="headerlink" title="1.3 Example"></a>1.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/1.3.png" alt="1.3"></p><h3 id="1-4-Leaderboard"><a href="#1-4-Leaderboard" class="headerlink" title="1.4 Leaderboard"></a>1.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/1.4.png" alt="1.4"></p><h3 id="1-5-Reference"><a href="#1-5-Reference" class="headerlink" title="1.5 Reference"></a>1.5 Reference</h3><p><a href="http://text-machine.cs.uml.edu/lab2/projects/quail/" target="_blank" rel="noopener">http://text-machine.cs.uml.edu/lab2/projects/quail/</a></p><h2 id="2-LogiQA-A-Challenge-Dataset-for-Machine-Reading-Comprehension-with-Logical-Reasoning-IJCAI-2020"><a href="#2-LogiQA-A-Challenge-Dataset-for-Machine-Reading-Comprehension-with-Logical-Reasoning-IJCAI-2020" class="headerlink" title="2. LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning, IJCAI 2020"></a>2. LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning, IJCAI 2020</h2><h3 id="2-1-Author"><a href="#2-1-Author" class="headerlink" title="2.1 Author"></a>2.1 Author</h3><p>Jian Liu–Fudan University<br>Yue Zhang–Westlake University</p><h3 id="2-2-Description"><a href="#2-2-Description" class="headerlink" title="2.2 Description"></a>2.2 Description</h3><p>LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning.<br>LogiQA by collecting the logical comprehension problems from publically available questions of the National Civil Servants Examination of China.</p><h3 id="2-3-Example"><a href="#2-3-Example" class="headerlink" title="2.3 Example"></a>2.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/2.3.png" alt="2.3"></p><h3 id="2-4-Leaderboard"><a href="#2-4-Leaderboard" class="headerlink" title="2.4 Leaderboard"></a>2.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/2.4.png" alt="2.4"></p><h3 id="2-5-Reference"><a href="#2-5-Reference" class="headerlink" title="2.5 Reference"></a>2.5 Reference</h3><p><a href="https://github.com/lgw863/LogiQA-dataset" target="_blank" rel="noopener">https://github.com/lgw863/LogiQA-dataset</a></p><h2 id="3-XCOPA-A-Multilingual-Dataset-for-Causal-Commonsense-Reasoning-EMNLP2020"><a href="#3-XCOPA-A-Multilingual-Dataset-for-Causal-Commonsense-Reasoning-EMNLP2020" class="headerlink" title="3. XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning, EMNLP2020"></a>3. XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning, EMNLP2020</h2><h3 id="3-1-Author"><a href="#3-1-Author" class="headerlink" title="3.1 Author"></a>3.1 Author</h3><p>Anna Korhonen:University of Cambridge, UK</p><h3 id="3-2-Description"><a href="#3-2-Description" class="headerlink" title="3.2 Description"></a>3.2 Description</h3><p>The Cross-lingual Choice of Plausible Alternatives dataset  is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around the globe. </p><h3 id="3-3-Example"><a href="#3-3-Example" class="headerlink" title="3.3 Example"></a>3.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/3.3.png" alt="3.3"></p><h3 id="3-4-Leaderboard"><a href="#3-4-Leaderboard" class="headerlink" title="3.4 Leaderboard"></a>3.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/3.4.png" alt="3.4"></p><h3 id="3-5-Reference"><a href="#3-5-Reference" class="headerlink" title="3.5 Reference"></a>3.5 Reference</h3><p><a href="https://github.com/cambridgeltl/xcopa" target="_blank" rel="noopener">https://github.com/cambridgeltl/xcopa</a></p><h2 id="4-WinoWhy-A-Deep-Diagnosis-of-Essential-Commonsense-Knowledge-for-Answering-Winograd-Schema-Challenge-ACL2020"><a href="#4-WinoWhy-A-Deep-Diagnosis-of-Essential-Commonsense-Knowledge-for-Answering-Winograd-Schema-Challenge-ACL2020" class="headerlink" title="4. WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge, ACL2020"></a>4. WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge, ACL2020</h2><h3 id="4-1-Author"><a href="#4-1-Author" class="headerlink" title="4.1 Author"></a>4.1 Author</h3><p>Yangqiu Song,Department of CSE, HKUST</p><h3 id="4-2-Description"><a href="#4-2-Description" class="headerlink" title="4.2 Description"></a>4.2 Description</h3><p>Given a pronoun coreference resolution question and its correct answer from the original WSC data, models are asked to select all plausible reasons for making the correct prediction.<br>This benchmark include the original Winograd Schema Challenge (WSC) dataset and 4095 WinoWhy reasons (15 for each WSC question) that could justify the pronoun coreference choices in WSC. WinoWhy contains 3 sources of reasons: (1) Human; (2) Human Reverse; (3) Generation Model. Each WSC reason has 5 reasons from each source.</p><p>Collected reasons are then used to categorize what types of commonsense knowledge are needed to solve the WSC question. The selected knowledge types are as follows (notice that a question could require knowledge from multiple categories):<br><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/4.2.png" alt="4.2"></p><h3 id="4-3-Example"><a href="#4-3-Example" class="headerlink" title="4.3 Example"></a>4.3 Example</h3><p>Question:”The city councilmen refused the demonstrators a permit because they feared violence. Does the ‘they’ refer to ‘the city councilmen’ or ‘the demonstrators’?”.The reasons are based on the question “The ‘they’ refers to the city councilmen because…”. The paired question of this WSC changes “feared” to “advocated”.<br><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/4.3.png" alt="4.3"></p><h3 id="4-4-Leaderboard"><a href="#4-4-Leaderboard" class="headerlink" title="4.4 Leaderboard"></a>4.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/4.4.png" alt="4.4"></p><h3 id="4-5-Reference"><a href="#4-5-Reference" class="headerlink" title="4.5 Reference"></a>4.5 Reference</h3><p><a href="https://github.com/HKUST-KnowComp/WinoWhy" target="_blank" rel="noopener">https://github.com/HKUST-KnowComp/WinoWhy</a></p><h2 id="5-WINOGRANDE-An-Adversarial-Winograd-Schema-Challenge-at-Scale-AAAI2020"><a href="#5-WINOGRANDE-An-Adversarial-Winograd-Schema-Challenge-at-Scale-AAAI2020" class="headerlink" title="5. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale, AAAI2020"></a>5. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale, AAAI2020</h2><h3 id="5-1-Author"><a href="#5-1-Author" class="headerlink" title="5.1 Author"></a>5.1 Author</h3><p>Yejin Choi,Allen Institute for Artiﬁcial Intelligence † University of Washington</p><h3 id="5-2-Description"><a href="#5-2-Description" class="headerlink" title="5.2 Description"></a>5.2 Description</h3><p>WINOG RANDE, a new dataset with 44k problems that are inspired by the original design of WSC, but modiﬁed to improve both the scale and hardness of the problems.</p><h3 id="5-3-Example"><a href="#5-3-Example" class="headerlink" title="5.3 Example"></a>5.3 Example</h3><p>QID:37SOB9Z0SUBUPIQRB8ROANMHJ463L8-1<br> Sentence: “She remembered how annoying it is to dust her wood chair so she bought a plastic table instead.  Cleaning the _ is time consuming. Option1: “chair”, “option2”: “table”<br>Answer: “1”</p><h3 id="5-4-Leaderboard"><a href="#5-4-Leaderboard" class="headerlink" title="5.4 Leaderboard"></a>5.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/5.4.png" alt="5.4"></p><h3 id="5-5-Reference"><a href="#5-5-Reference" class="headerlink" title="5.5 Reference"></a>5.5 Reference</h3><p><a href="https://winogrande.allenai.org" target="_blank" rel="noopener">https://winogrande.allenai.org</a></p><h2 id="6-Does-It-Make-Sense-And-Why-A-Pilot-Study-for-Sense-Making-and-Explanation-ACL2019"><a href="#6-Does-It-Make-Sense-And-Why-A-Pilot-Study-for-Sense-Making-and-Explanation-ACL2019" class="headerlink" title="6. Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation, ACL2019"></a>6. Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation, ACL2019</h2><h3 id="6-1-Author"><a href="#6-1-Author" class="headerlink" title="6.1 Author"></a>6.1 Author</h3><p>Cunxiang Wang,Tian Gao–Zhejiang University,</p><h3 id="6-2-Description"><a href="#6-2-Description" class="headerlink" title="6.2 Description"></a>6.2 Description</h3><p>A benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense.</p><h3 id="6-3-Example"><a href="#6-3-Example" class="headerlink" title="6.3 Example"></a>6.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/6.3.png" alt="6.3"></p><h3 id="6-4-LeaderBoard"><a href="#6-4-LeaderBoard" class="headerlink" title="6.4 LeaderBoard"></a>6.4 LeaderBoard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/6.4.png" alt="6.4"></p><h3 id="6-5-Reference"><a href="#6-5-Reference" class="headerlink" title="6.5 Reference"></a>6.5 Reference</h3><p><a href="https://competitions.codalab.org/competitions/21080" target="_blank" rel="noopener">https://competitions.codalab.org/competitions/21080</a></p><h2 id="7-PIQA-Reasoning-about-Physical-Commonsense-in-Natural-Language-AAAI2020-AI2"><a href="#7-PIQA-Reasoning-about-Physical-Commonsense-in-Natural-Language-AAAI2020-AI2" class="headerlink" title="7. PIQA: Reasoning about Physical Commonsense in Natural Language, AAAI2020, AI2"></a>7. PIQA: Reasoning about Physical Commonsense in Natural Language, AAAI2020, AI2</h2><h3 id="7-1-Author"><a href="#7-1-Author" class="headerlink" title="7.1 Author"></a>7.1 Author</h3><p>Yonatan Bisk,Yejin Choi, Allen Institute for Artiﬁcial Intelligence,Carnegie Mellon University,University of Washington</p><h3 id="7-2-Description"><a href="#7-2-Description" class="headerlink" title="7.2 Description"></a>7.2 Description</h3><p>PIQA , for benchmarking progress in <strong>physical commonsense understanding</strong>. The underlying task is multiple choice question answering: given a question q and two possible solutions s1 , s2 , a model or a human must choose the most appropriate solution, of which exactly one is correct.</p><h3 id="7-3-Example"><a href="#7-3-Example" class="headerlink" title="7.3 Example"></a>7.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/7.3.png" alt="7.3"></p><h3 id="7-4-Leaderboard"><a href="#7-4-Leaderboard" class="headerlink" title="7.4 Leaderboard"></a>7.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/7.4.png" alt="7.4"></p><h3 id="7-5-Reference"><a href="#7-5-Reference" class="headerlink" title="7.5 Reference"></a>7.5 Reference</h3><p><a href="https://yonatanbisk.com/piqa/" target="_blank" rel="noopener">https://yonatanbisk.com/piqa/</a></p><h2 id="8-COSMOS-QA-Machine-Reading-Comprehension-with-Contextual-Commonsense-Reasoning-EMNLP2019-AI2"><a href="#8-COSMOS-QA-Machine-Reading-Comprehension-with-Contextual-Commonsense-Reasoning-EMNLP2019-AI2" class="headerlink" title="8. COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning, EMNLP2019, AI2"></a>8. COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning, EMNLP2019, AI2</h2><h3 id="8-1-Author"><a href="#8-1-Author" class="headerlink" title="8.1 Author"></a>8.1 Author</h3><p>Lifu Huang, University of Illinois Urbana-Champaign<br>Yejin Choi, AI2</p><h3 id="8-2-Description"><a href="#8-2-Description" class="headerlink" title="8.2 Description"></a>8.2 Description</h3><p>A large-scale dataset of 35, 600 problems that require commonsense-based <strong>reading comprehension</strong>, formulated as multiple-choice questions. This dataset focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking such questions as “what might be the possible reason of …?”, or “what would have happened if …” that require reasoning beyond the exact text spans in the context.</p><h3 id="8-3-Example"><a href="#8-3-Example" class="headerlink" title="8.3 Example"></a>8.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/8.3.png" alt="8.3"></p><h3 id="8-4-Leaderboard"><a href="#8-4-Leaderboard" class="headerlink" title="8.4 Leaderboard"></a>8.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/8.4.png" alt="8.4"></p><h3 id="8-5-Reference"><a href="#8-5-Reference" class="headerlink" title="8.5 Reference"></a>8.5 Reference</h3><p>Code&amp;Dadaset–<a href="https://wilburone.github.io/cosmos/" target="_blank" rel="noopener">https://wilburone.github.io/cosmos/</a></p><h2 id="9-SOCIALIQA-Commonsense-Reasoning-about-Social-Interactions-EMNLP2019-AI2"><a href="#9-SOCIALIQA-Commonsense-Reasoning-about-Social-Interactions-EMNLP2019-AI2" class="headerlink" title="9. SOCIALIQA: Commonsense Reasoning about Social Interactions, EMNLP2019, AI2"></a>9. SOCIALIQA: Commonsense Reasoning about Social Interactions, EMNLP2019, AI2</h2><h3 id="9-1-Author"><a href="#9-1-Author" class="headerlink" title="9.1 Author"></a>9.1 Author</h3><p>Maarten Sap, Yejin Choi</p><h3 id="9-2-Description"><a href="#9-2-Description" class="headerlink" title="9.2 Description"></a>9.2 Description</h3><p>SOCIALIQA aims to measure the social and emotional intelligence of computational models through multiple choice question answering (QA).<br>The ﬁrst large-scale benchmark for commonsense reasoning about social situations, containing over 38k QA pairs.</p><h3 id="9-3-Example"><a href="#9-3-Example" class="headerlink" title="9.3 Example"></a>9.3 Example</h3><ul><li>{“context”: “Even though she had homework to do that night, Jesse helped Skylar study.”, “question”: “What will Jesse want to do next?”, “answerA”: “read homework to Skylar”, “answerB”: “help Skylar finish”, “answerC”: “skip her studying”, “correct”: “B”}</li><li>{“context”: “After school, Casey met the friend at a bar so they could study for the final together.”, “question”: “Why did Casey do this?”, “answerA”: “have a good idea of the material”, “answerB”: “goof around with a friend”, “answerC”: “have a few drinks and leave”, “correct”: “A”}</li></ul><h3 id="9-4-Leaderboard"><a href="#9-4-Leaderboard" class="headerlink" title="9.4 Leaderboard"></a>9.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/9.4.png" alt="9.4"></p><h3 id="9-5-Reference"><a href="#9-5-Reference" class="headerlink" title="9.5 Reference"></a>9.5 Reference</h3><p><a href="https://maartensap.github.io/social-iqa/" target="_blank" rel="noopener">https://maartensap.github.io/social-iqa/</a></p><h2 id="10-COMMONSENSEQA-A-Question-Answering-Challenge-Targeting-Commonsense-Knowledge-NAACL2019-AI2"><a href="#10-COMMONSENSEQA-A-Question-Answering-Challenge-Targeting-Commonsense-Knowledge-NAACL2019-AI2" class="headerlink" title="10. COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge, NAACL2019, AI2"></a>10. COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge, NAACL2019, AI2</h2><h3 id="10-1-Author"><a href="#10-1-Author" class="headerlink" title="10.1 Author"></a>10.1 Author</h3><p>Alon Talmor, Jonathan Berant–Allen Institute for Artiﬁcial Intelligence;Tel-Aviv University</p><h3 id="10-2-Description"><a href="#10-2-Description" class="headerlink" title="10.2 Description"></a>10.2 Description</h3><p>CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.The best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.</p><h3 id="10-3-Example"><a href="#10-3-Example" class="headerlink" title="10.3 Example"></a>10.3 Example</h3><p>A crowd worker observes a source concept (‘River’ in Figure 1) and three target concepts (‘Waterfall’, ‘Bridge’, ‘Valley’) that are all related by the same C ONCEPTN ET relation (AtLocation).<br><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/10.3.png" alt="10.3"></p><h3 id="10-4-Leaderboard"><a href="#10-4-Leaderboard" class="headerlink" title="10.4 Leaderboard"></a>10.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/10.4.png" alt="10.4"></p><h3 id="10-5-Reference"><a href="#10-5-Reference" class="headerlink" title="10.5 Reference"></a>10.5 Reference</h3><p><a href="https://www.tau-nlp.org/csqa-leaderboard" target="_blank" rel="noopener">https://www.tau-nlp.org/csqa-leaderboard</a></p><h2 id="11-MathQA-Towards-Interpretable-Math-Word-Problem-Solving-with-Operation-Based-Formalisms-NAACL-2019-AI2"><a href="#11-MathQA-Towards-Interpretable-Math-Word-Problem-Solving-with-Operation-Based-Formalisms-NAACL-2019-AI2" class="headerlink" title="11. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms, NAACL, 2019, AI2"></a>11. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms, NAACL, 2019, AI2</h2><h3 id="11-1-Author"><a href="#11-1-Author" class="headerlink" title="11.1 Author"></a>11.1 Author</h3><p>Aida Amini–University of Washington<br>Hannaneh Hajishirzi–Allen Institute for AI</p><h3 id="11-2-Description"><a href="#11-2-Description" class="headerlink" title="11.2 Description"></a>11.2 Description</h3><p>MathQA, a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017).</p><h3 id="11-3-Example"><a href="#11-3-Example" class="headerlink" title="11.3 Example"></a>11.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/11.3.png" alt="11.3"></p><h3 id="11-4-Leaderboard"><a href="#11-4-Leaderboard" class="headerlink" title="11.4 Leaderboard"></a>11.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/11.4.png" alt="11.4"></p><h3 id="11-5-Reference"><a href="#11-5-Reference" class="headerlink" title="11.5 Reference"></a>11.5 Reference</h3><p><a href="https://math-qa.github.io/math-QA/" target="_blank" rel="noopener">https://math-qa.github.io/math-QA/</a></p><h2 id="12-HellaSwag-Can-a-Machine-Really-Finish-Your-Sentence-ACL2019-AI2"><a href="#12-HellaSwag-Can-a-Machine-Really-Finish-Your-Sentence-ACL2019-AI2" class="headerlink" title="12. HellaSwag: Can a Machine Really Finish Your Sentence?, ACL2019, AI2"></a>12. HellaSwag: Can a Machine Really Finish Your Sentence?, ACL2019, AI2</h2><h3 id="12-1-Author"><a href="#12-1-Author" class="headerlink" title="12.1 Author"></a>12.1 Author</h3><p>Rowan Zellers–University of Washington<br>Yejin Choi–Allen Institute for Artiﬁcial Intelligence</p><h3 id="12-2-Description"><a href="#12-2-Description" class="headerlink" title="12.2 Description"></a>12.2 Description</h3><p>HellaSwag, a new benchmark for commonsense NLI. Its questions are trivial for humans (95% accuracy), state-of-the-art models struggle (48%).</p><h3 id="12-3-Example"><a href="#12-3-Example" class="headerlink" title="12.3 Example"></a>12.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/12.3.png" alt="12.3"></p><h3 id="12-4-Leaderboard"><a href="#12-4-Leaderboard" class="headerlink" title="12.4 Leaderboard"></a>12.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/12.4.png" alt="12.4"></p><h3 id="12-5-Reference"><a href="#12-5-Reference" class="headerlink" title="12.5 Reference"></a>12.5 Reference</h3><p><a href="https://rowanzellers.com/hellaswag/#leaderboard" target="_blank" rel="noopener">https://rowanzellers.com/hellaswag/#leaderboard</a><br>Dataset–<a href="https://github.com/rowanz/hellaswag/tree/master/data" target="_blank" rel="noopener">https://github.com/rowanz/hellaswag/tree/master/data</a></p><h2 id="13-Swag-A-Large-Scale-Adversarial-Dataset-for-Grounded-Commonsense-Inference-EMNLP2018-AI2"><a href="#13-Swag-A-Large-Scale-Adversarial-Dataset-for-Grounded-Commonsense-Inference-EMNLP2018-AI2" class="headerlink" title="13. Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference, EMNLP2018, AI2"></a>13. Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference, EMNLP2018, AI2</h2><h3 id="13-1-Author"><a href="#13-1-Author" class="headerlink" title="13.1 Author"></a>13.1 Author</h3><p>Rowan Zellers–University of Washington<br>Yejin Choi–Allen Institute for Artiﬁcial Intelligence</p><h3 id="13-2-Description"><a href="#13-2-Description" class="headerlink" title="13.2 Description"></a>13.2 Description</h3><p>SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.<br>The dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. We aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.</p><h3 id="13-3-Example"><a href="#13-3-Example" class="headerlink" title="13.3 Example"></a>13.3 Example</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/13.3.png" alt="13.3"></p><h3 id="13-4-Leaderboard"><a href="#13-4-Leaderboard" class="headerlink" title="13.4 Leaderboard"></a>13.4 Leaderboard</h3><p><img src="/2020/12/10/%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94Benchmarks/13.4.png" alt="13.4"></p><h3 id="13-5-Reference"><a href="#13-5-Reference" class="headerlink" title="13.5 Reference"></a>13.5 Reference</h3><p><a href="https://rowanzellers.com/swag/" target="_blank" rel="noopener">https://rowanzellers.com/swag/</a></p><h2 id="More-Reading"><a href="#More-Reading" class="headerlink" title="More Reading"></a>More Reading</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-QuAIL-Getting-Closer-to-AI-Complete-Question-Answering-A-Set-of-Prerequisite-Real-Tasks-AAAI2020&quot;&gt;&lt;a href=&quot;#1-QuAIL-Getting-Closer
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>融合外部知识的常识推理相关论文解读（一）</title>
    <link href="http://xiuyeliang.com/2020/11/20/Commonsense-Reasoning-1/"/>
    <id>http://xiuyeliang.com/2020/11/20/Commonsense-Reasoning-1/</id>
    <published>2020-11-20T07:13:41.000Z</published>
    <updated>2020-11-20T08:19:56.325Z</updated>
    
    <content type="html"><![CDATA[<h3 id="这里我们将介绍几篇关于常识推理的文章"><a href="#这里我们将介绍几篇关于常识推理的文章" class="headerlink" title="这里我们将介绍几篇关于常识推理的文章:"></a>这里我们将介绍几篇关于常识推理的文章:</h3><p>第一篇是Xiang Ren（University of Southern California）实验室组发表在EMNLP2019的《KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning》[1]。该篇文章通过使用外部知识库（Symbolic space）来实现在常识问答中的推理（Semantic Space）。提出的框架如下图所示，ConcetpNet数据集作为外部知识，首先根据ConcetpNet识别出问题与答案中的Concepts。然后构建与问题答案相关的上下文子图subgraph.为了捕获multi-hop关系，该文选择用LSTM编码问题concept与答案concept之间的路径。之后将LSTM的输出向量（Symbolic vector）与问答答案对（QA-pair）在预训练语言模型上获取到的向量（Statement vector）进行拼接，最后通过一个MLP网络给QA-pair打分。值得一提的是，该模型为了提供模型的可解释性。 提出了一种层次注意力机制（HPA）。分别考虑了路径级别和概念对级别的注意力机制。<br><img src="/2020/11/20/Commonsense-Reasoning-1/1.png" alt="1"></p><p>第二篇文章《Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering》[2]同样也是由Xiang Ren实验组发表在EMNLP2020。这篇文章的研究动机是：由于路径数量随着节点个数的增加而呈指数增长，因此已有的基于路径的常识推理方法虽然有很好的可解释性但是却很难扩展。然而，GCN具有很好的扩展性但是不具备可解释性。因此作者融合了二者的优点，提出了Multi-hop Graph Relation Network(MHGRN)多跳图关系网络模型。提出的模型框架如下图所示。通俗理解就是，该模型在GCN上定义了一个新的信息传播机制，使得当前节点可以捕获多跳关系信息并实现可解释性。<br><img src="/2020/11/20/Commonsense-Reasoning-1/2.png" alt="2"></p><p>第三篇文章《Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering》[3]发表在AAAI2020. 其主要是想通过融合不同源的外部知识（ConceptNet,WikiPedia）来提高模型常识知识推理的性能。其框架如下图所示，主要可以分为二个部分：第一步从不同源的外部知识获取问题答案的相关证据。并且根据相关性程度对证据句子进行排序（从而使得相近的concept很接近，以便利用上下文信息获得更好的word embedding.实验部分也验证了该步的有效性）。第二步是Graph-based Reasoning,主要是GCN对相关证据进行编码。这篇文章主要思想通过二种不同源外部知识来提取证据，但是不具备可解释性。<br><img src="/2020/11/20/Commonsense-Reasoning-1/3.png" alt="3"></p><p>第四篇文章《Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources》[4]是由浙江大学和阿里巴巴联合发表在COLING2020上的。该篇文章的思想比较简单，但是达到了当前(2020.11.20)SOTA的效果，在CommonsenseQA 任务上超过了排行榜第一名。该模型框架如下图所示。主要思想是利用了三种不同源的外部知识（ConceptNet,Wikipedia,Cambridge Dictionary）来提出问题答案对的证据，并且提出了一个Graph-based Iterative Retrieval 模块。Cambridge Dictionary数据集在这是第一次被用于作为常识推理的外部知识。个人认为，能取得SOTA效果是因为外部知识数据库大，知识类型多。该模型同样缺乏可解释性。<br><img src="/2020/11/20/Commonsense-Reasoning-1/4.png" alt="4"></p><p>第五篇文章《LEARNING CONTEXTUALIZED KNOWLEDGE STRUCTURES FOR COMMONSENSE REASONING》[5] 同样也是Xiang Ren实验组投稿在ICLR2021（目前得分5，5，7）。这篇文章的研究动机为：作者认为基于外部知识获取的sub-graph具有较低的事实覆盖率并且存在噪声。为了解决这些问题，作者提出来联合从外部知识抽取事实以及生成事实，由于生成的事实可能是噪声，因此基于graph边的权重对其进行剪裁。作者在图中定义了二种（edge-&gt;node和node-&gt;edge）信息传播机制来实现推理。同样，该方法不具备可解释性。<br><img src="/2020/11/20/Commonsense-Reasoning-1/5.png" alt="5"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Lin, Bill Yuchen, et al. “Kagnet: Knowledge-aware graph networks for commonsense reasoning.” arXiv preprint arXiv:1909.02151 (2019).</li><li>Feng, Yanlin, et al. “Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering.” arXiv preprint arXiv:2005.00646 (2020).</li><li>Lv, Shangwen, et al. “Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering.” AAAI. 2020.</li><li>Chen, Qianglong, et al. “Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources.” arXiv preprint arXiv:2011.02705 (2020).</li><li>Yan, Jun, et al. “Learning Contextualized Knowledge Structures for Commonsense Reasoning.” arXiv preprint arXiv:2010.12873 (2020).</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;这里我们将介绍几篇关于常识推理的文章&quot;&gt;&lt;a href=&quot;#这里我们将介绍几篇关于常识推理的文章&quot; class=&quot;headerlink&quot; title=&quot;这里我们将介绍几篇关于常识推理的文章:&quot;&gt;&lt;/a&gt;这里我们将介绍几篇关于常识推理的文章:&lt;/h3&gt;&lt;p&gt;第一篇是
      
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>label_word_attention</title>
    <link href="http://xiuyeliang.com/2020/06/12/label-word-attention/"/>
    <id>http://xiuyeliang.com/2020/06/12/label-word-attention/</id>
    <published>2020-06-12T14:38:38.000Z</published>
    <updated>2020-06-12T14:38:38.831Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>HEDGE</title>
    <link href="http://xiuyeliang.com/2020/06/05/Generating%20Hierarchical%20Explanations%20on%20Text%20Classi%EF%AC%81cation%20via%20Feature%20Interaction%20Detection/"/>
    <id>http://xiuyeliang.com/2020/06/05/Generating%20Hierarchical%20Explanations%20on%20Text%20Classi%EF%AC%81cation%20via%20Feature%20Interaction%20Detection/</id>
    <published>2020-06-05T07:08:10.000Z</published>
    <updated>2020-06-05T07:13:18.094Z</updated>
    
    <content type="html"><![CDATA[<div class="row">    <embed src="./HEDGE.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      
      
        

	&lt;div class=&quot;row&quot;&gt;
    &lt;embed src=&quot;./HEDGE.pdf&quot; width=&quot;100%&quot; height=&quot;550&quot; type=&quot;application/pdf&quot;&gt;
	&lt;/div&gt;



      
    
    </summary>
    
    
      <category term="论文阅读" scheme="http://xiuyeliang.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="自然语言处理" scheme="http://xiuyeliang.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>阅读《A Survey on Knowledge Graphs-Representation, Acquisition and Applications》-AAAI2020</title>
    <link href="http://xiuyeliang.com/2020/05/06/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0ICLR2020/"/>
    <id>http://xiuyeliang.com/2020/05/06/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0ICLR2020/</id>
    <published>2020-05-06T09:52:57.000Z</published>
    <updated>2020-05-08T02:15:56.697Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>该综述主要从4个方面对知识图谱相关信息进行总结。</p><ol><li>知识图谱表示-主要是对图中的实体和关系用低维向量表示。</li></ol><p><img src="/2020/05/06/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0ICLR2020/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0.png" alt="知识图谱综述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;p&gt;该综述主要从4个方面对知识图谱相关信息进行总结。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;知识图谱表示-主要是对图中的实体和关系用低维
      
    
    </summary>
    
    
      <category term="论文阅读" scheme="http://xiuyeliang.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="自然语言处理" scheme="http://xiuyeliang.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="知识图谱" scheme="http://xiuyeliang.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
  </entry>
  
  <entry>
    <title>逻辑回归模型</title>
    <link href="http://xiuyeliang.com/2020/05/04/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://xiuyeliang.com/2020/05/04/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-05-04T02:54:35.000Z</published>
    <updated>2020-05-14T09:49:59.316Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><p>线性回归的表达式： $y(x, w)=w_{0}+w_{1} x_{1}+\ldots+w_{n} x_{n}$</p><h2 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2. 逻辑回归"></a>2. 逻辑回归</h2><h3 id="2-1逻辑回归的定义"><a href="#2-1逻辑回归的定义" class="headerlink" title="2.1逻辑回归的定义"></a>2.1逻辑回归的定义</h3><p>线性回归可以预测连续值，但是不能解决分类问题，我们需要根据预测的结果判定其属于正类还是负类。所以逻辑回归就是将线性回归的(−∞,+∞)<br>(−∞,+∞)结果，通过sigmoid函数映射到(0,1)<br>(0,1)之间。<br><img src="/2020/05/04/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E5%9B%BE.png" alt="逻辑回归函数图"></p><p><strong>为什么使用sigmoid函数？</strong></p><ol><li>可以对(−∞,+∞)结果，映射到(0,1)之间，作为概率。</li><li>$x&lt;0,$ sigmoid$(x)&lt;\frac{1}{2} ; x&gt;0,$ sigmoid $(x)&gt;\frac{1}{2}$ ，可以将1/2作为决策边界。</li><li>数学特性好，求导容易： $g^{\prime}(z)=g(z) \cdot(1-g(z))$</li></ol><p>线性回归决策函数： $h \theta(x)=\theta^{T} x$<br>将其通过sigmoid函数，获得逻辑回归的决策函数： $h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} x}}$<br>逻辑回归可以写成：<br>$P(y=1 | x ; \theta)=h_{\theta}(x)$<br>$P(y=0 | x ; \theta)=1-h_{\theta}(x)$<br>然后可以写成统一的形式：<br>$p(y | x ; \theta)=\left(h_{\theta}(x)\right)^{y}\left(1-h_{\theta}(x)\right)^{1-y}$</p><h3 id="2-2逻辑回归损失函数"><a href="#2-2逻辑回归损失函数" class="headerlink" title="2.2逻辑回归损失函数"></a>2.2逻辑回归损失函数</h3><p>由最大似然估计原理，我们可以通过m个训练样本值，来估计出值，使得似然函数值（所有样本的似然函数之积）最大：<br>$\begin{aligned} L(\theta) &amp;=p(\vec{y} | X ; \theta) \ &amp;=\prod_{i=1}^{m} p\left(y^{(i)} | x^{(i)} ; \theta\right) \ &amp;=\prod_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}} \end{aligned}$<br>求log：<br>$\begin{aligned} \ell(\theta) &amp;=\log L(\theta) \ &amp;=\sum_{i=1}^{m} y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right) \end{aligned}$<br>取负数，得损失函数：<br>$J(\theta)=\frac{1}{m} \sum_{i=1}^{n} \operatorname{cost}\left(h_{\theta}\left(x_{i}\right), y_{i}\right)=-\frac{1}{m}\left[\sum_{i=1}^{n} y_{i} \log h_{\theta}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{\theta}\left(x_{i}\right)\right)\right]$</p><p>Question: 为什么逻辑回归采用似然函数，而不是平方损失函数？</p><ol><li>最小二乘的损失函数是非凸函数。没有全局最优解。</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://blog.csdn.net/jiaoyangwm/article/details/81139362" target="_blank" rel="noopener">https://blog.csdn.net/jiaoyangwm/article/details/81139362</a></li><li><a href="https://blog.csdn.net/ddydavie/article/details/82668141" target="_blank" rel="noopener">https://blog.csdn.net/ddydavie/article/details/82668141</a></li><li><a href="https://blog.csdn.net/u012421852/article/details/79575720" target="_blank" rel="noopener">https://blog.csdn.net/u012421852/article/details/79575720</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-线性回归&quot;&gt;&lt;a href=&quot;#1-线性回归&quot; class=&quot;headerlink&quot; title=&quot;1. 线性回归&quot;&gt;&lt;/a&gt;1. 线性回归&lt;/h2&gt;&lt;p&gt;线性回归的表达式： $y(x, w)=w_{0}+w_{1} x_{1}+\ldots+w_{n} x_
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归与逻辑回归" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
    
  </entry>
  
  <entry>
    <title>隐马尔可夫模型</title>
    <link href="http://xiuyeliang.com/2020/05/03/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E2%80%98/"/>
    <id>http://xiuyeliang.com/2020/05/03/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E2%80%98/</id>
    <published>2020-05-03T13:33:59.000Z</published>
    <updated>2020-05-04T02:33:29.970Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-马尔科夫过程"><a href="#1-马尔科夫过程" class="headerlink" title="1. 马尔科夫过程"></a>1. 马尔科夫过程</h2><p>在已知的目前状态条件下，它未来的演变不依赖于它以往的演变。马尔科夫过程包括一个初始向量和一个状态转移矩阵。</p><h2 id="2-马尔科夫链"><a href="#2-马尔科夫链" class="headerlink" title="2. 马尔科夫链"></a>2. 马尔科夫链</h2><p>时间和状态都是离散的马尔科夫过程称为马尔科夫链。</p><h2 id="3-隐马尔可夫模型"><a href="#3-隐马尔可夫模型" class="headerlink" title="3. 隐马尔可夫模型"></a>3. 隐马尔可夫模型</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><p>在某些情况下马尔科夫过程不足以描述我们希望发现的模式。譬如，一个隐居的人可能不能直观的观察到天气的情况，但是有一些海藻。民间的传说告诉我们海藻的状态在某种概率上是和天气的情况相关的。在这种情况下我们有两个状态集合，一个可以观察到的状态集合（海藻的状态）和一个隐藏的状态（天气的状况）。我们希望能找到一个算法可以根据海藻的状况和马尔科夫假设来预测天气的状况。</p><p>其中，隐藏状态的数目和可以观察到的状态的数目可能是不一样的。在语音识别中，一个简单的发言也许只需要80个语素来描述，但是一个内部的发音机制可以产生不到80或者超过80种不同的声音。同理，在一个有三种状态的天气系统（sunny、cloudy、rainy）中，也许可以观察到四种潮湿程度的海藻（dry、dryish、damp、soggy）。在此情况下，可以观察到的状态序列和隐藏的状态序列是概率相关的。于是我们可以将这种类型的过程建模为一个隐藏的马尔科夫过程和一个和这个马尔科夫过程概率相关的并且可以观察到的状态集合。</p><h3 id="3-2-隐马尔科夫"><a href="#3-2-隐马尔科夫" class="headerlink" title="3.2 隐马尔科夫"></a>3.2 隐马尔科夫</h3><h4 id="3-2-1-隐马尔可夫模型表示"><a href="#3-2-1-隐马尔可夫模型表示" class="headerlink" title="3.2.1 隐马尔可夫模型表示"></a>3.2.1 隐马尔可夫模型表示</h4><p>HMM由隐含状态S,可观测状态O,初始状态概率矩阵pi,隐含状态概率转移矩阵A，可观测转移矩阵B（<strong>发射矩阵</strong>）组成。pi和A决定了状态序列，B决定了观测序列，因此，HMM可以由三元符号表示：<br>$\lambda=(A, B, \pi)$</p><h4 id="3-2-2-HMM的二个性质"><a href="#3-2-2-HMM的二个性质" class="headerlink" title="3.2.2 HMM的二个性质"></a>3.2.2 HMM的二个性质</h4><ul><li>齐次假设-假设t时刻与t-1之前的所有状态和观测是独立的。<br>$P\left(i_{t} | i_{t-1}, o_{t-1}, i_{t-2}, o_{t-2} \cdots i_{1}, o_{1}\right)=P\left(i_{t} | i_{t-1}\right)$</li><li>观测独立性假设-t时刻的观测ot与t时刻之前的所有状态和观测是独立的。<br>$P\left(o_{t} | i_{T}, o_{T}, i_{T-1}, o_{T-1} \cdots i_{1}, o_{1}\right)=P\left(o_{t} | i_{t}\right)$<h4 id="HMM的三个问题"><a href="#HMM的三个问题" class="headerlink" title="HMM的三个问题"></a>HMM的三个问题</h4></li></ul><ol><li>概率计算问题：给定模型 $\lambda=(A, B, \pi)$ 和观测序列 $O={O_1,O_2,O_3 \dots}$ ，计算在模型 $\lambda$ 下观测 $O$ 出现的概率 $P(O|\lambda)$ .（解决算法–&gt;前向-后向算法（动态规划））。</li><li>学习问题：已知观测序列 $O={O_1,O_2,O_3 \dots}$ ，估计模型 $\lambda=(A, B, \pi)$ 的参数，使得在该参数下该模型的观测序列 $P(O|\lambda)$ 最大。（解决算法–Baum-Welch算法-EM算法）。</li><li>预测问题（解码问题）：已知模型 $\lambda=(A, B, \pi)$ 和观测序列 $O={O_1,O_2,O_3 \dots}$ ，求给定观测序列条件概率 $P(I|O,\lambda)$ 最大的状态序列 $I$ 。</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.cnblogs.com/pinking/p/8531405.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinking/p/8531405.html</a></li><li><a href="https://blog.csdn.net/z2536083458/article/details/99696875" target="_blank" rel="noopener">https://blog.csdn.net/z2536083458/article/details/99696875</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-马尔科夫过程&quot;&gt;&lt;a href=&quot;#1-马尔科夫过程&quot; class=&quot;headerlink&quot; title=&quot;1. 马尔科夫过程&quot;&gt;&lt;/a&gt;1. 马尔科夫过程&lt;/h2&gt;&lt;p&gt;在已知的目前状态条件下，它未来的演变不依赖于它以往的演变。马尔科夫过程包括一个初始向量和
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="隐马尔可夫模型" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>维特比(Viterbi)算法及python实现</title>
    <link href="http://xiuyeliang.com/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/"/>
    <id>http://xiuyeliang.com/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/</id>
    <published>2020-05-03T08:30:19.000Z</published>
    <updated>2020-05-03T10:02:50.679Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-维特比算法"><a href="#1-维特比算法" class="headerlink" title="1. 维特比算法"></a>1. 维特比算法</h2><p>维特比算法是一种<strong>动态规划</strong>苏算法，可用于最可能产生观测时间序列的-维特比路径-隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔可夫模型中。</p><p>在计算机科学领域中，动态规划的思想解决的最基本的一个问题是：寻找有向图无环图当中二个点之间的最短路径（实际应用于语音识别，词性标注，分词等）</p><h2 id="2-例子"><a href="#2-例子" class="headerlink" title="2. 例子"></a>2. 例子</h2><p>如下图所示，求S-&gt;E的最短路径，<br><img src="/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/%E6%88%AA%E5%B1%8F2020-05-03%2016.42.34.png" alt="截屏2020-05-03 16.42.34"></p><ol><li>最简单的事穷举法，把所有可能的路径都举出来，即$4<em>4</em>4=64$ 种可能。</li><li>基于动态规划的方式来寻找最佳路径：思想是把大的问题细分为多个小的问题，基于每一步的结果再去寻找下一步的策略，通过每一步走过之后的局部最优去寻找全局最优，<br>以下是具体做法：<br><img src="/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/%E6%88%AA%E5%B1%8F2020-05-03%2016.43.12.png" alt="截屏2020-05-03 16.43.12"></li></ol><p><strong>step 1</strong>: 从点S出发，对于第一层的4个节点，算出他们的距离 d(S,A1)，d(S,A2),d(S,A3),d(S,A4),因为只有一步，所以这些距离都是S到他们的最短距离。</p><p><strong>step 2</strong>: 对于B层的所有节点(B1,B2,B3,B4),要计算出S到他们的最短距离。我们知道，对于特定的节点B2，从S到它的路径可以经过A层的任何一个节点(A1,A2,A3,A4)。对应的路径长就是d(S,B2)=d(S,Ai)+d(Ai,B2)（其中i=1，2，3，4）。由于A层有4个节点（即i有4个取值），我们要一一计算，然后找到最小值。这样，对于B层的每个节点，都需要进行4次运算，而B层有4个节点，所以共有 $4*4=16$ 次运算。</p><p><strong>step 3</strong>:  这一步是该算法的核心。我们从step2计算得出的阶段结果只保留4个最短路径值（每个节点保留一个）。那么，若从B层走向C层来说，该步骤的级数已经不再是 16 ，而是变成4！也就是说，从B层到C层的最短路径只需要基于B层得出的4个结果来计算。这种方法一直持续到最后一个状态，每一步计算的复杂度为相邻两层的计算复杂度为 16的正比！再通俗点说，连接着两两相邻层的计算符合变成了+号，取代了原先的 * 号。用这种方法，只需要进行32 次计算！</p><p>上述就是著名的维特比算法，<br>若假设整个网络的宽度为D，网格长度为N，那么穷举法的时间复杂度为 $O(D^N)$ , 而维特比算法的时间复杂度为 $O(ND^2)$.</p><h2 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h2><p>以词性标注为例，利用维特比算法对一个句子的词性进行标注。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#第一个参数表示转移概率矩阵 $ P(Z_i|Z_&#123;i-1&#125;)$,第二个参数表示发射概率$P(W_i|Z_i)$,第三个参数表示隐藏变量Z的初始概率 $P(Z_i)$,第四个参数表示要标注的句子对像。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(trainsition_probability,emission_probability,pi,obs_seq)</span>:</span></span><br><span class="line">    <span class="comment">#转换为矩阵进行运算</span></span><br><span class="line">    trainsition_probability=np.array(trainsition_probability)</span><br><span class="line">    emission_probability=np.array(emission_probability)</span><br><span class="line">    pi=np.array(pi)</span><br><span class="line">    obs_seq = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>]  <span class="comment">#句子中的词在词库中的下标位置</span></span><br><span class="line">    <span class="comment"># 最后返回一个Row*Col的矩阵结果</span></span><br><span class="line">    Row = np.array(trainsition_probability).shape[<span class="number">0</span>]<span class="comment">#获取词性的个数</span></span><br><span class="line">    Col = len(obs_seq)<span class="comment">#获取句子中词的个数</span></span><br><span class="line">    <span class="comment">#定义要返回的矩阵，即动态规划中需要维护的矩阵，计算矩阵中的每一个元素值</span></span><br><span class="line">    F=np.zeros((Row,Col))<span class="comment">#行数代表词的个数，列数代表词性的种类</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始状态</span></span><br><span class="line">F[:,<span class="number">0</span>]=pi*np.transpose(emission_probability[:,obs_seq[<span class="number">0</span>]])</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>,Col):<span class="comment">#针对每一列</span></span><br><span class="line">        list_max=[]</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(Row):<span class="comment">#遍历每一行</span></span><br><span class="line">            list_x=list(np.array(F[:,t<span class="number">-1</span>])*np.transpose(trainsition_probability[:,n]))</span><br><span class="line">            <span class="comment">#获取最大概率</span></span><br><span class="line">            list_p=[]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> list_x:</span><br><span class="line">                list_p.append(i*<span class="number">10000</span>)</span><br><span class="line">            list_max.append(max(list_p)/<span class="number">10000</span>)</span><br><span class="line">        F[:,t]=np.array(list_max)*np.transpose(emission_probability[:,obs_seq[t]])</span><br><span class="line">    <span class="keyword">return</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#隐藏状态</span></span><br><span class="line">    invisible=[<span class="string">'Sunny'</span>,<span class="string">'Cloud'</span>,<span class="string">'Rainy'</span>]</span><br><span class="line">    <span class="comment">#初始状态</span></span><br><span class="line">    pi=[<span class="number">0.63</span>,<span class="number">0.17</span>,<span class="number">0.20</span>]</span><br><span class="line">    <span class="comment">#转移矩阵</span></span><br><span class="line">    trainsion_probility=[[<span class="number">0.5</span>,<span class="number">0.375</span>,<span class="number">0.125</span>],[<span class="number">0.25</span>,<span class="number">0.125</span>,<span class="number">0.625</span>],[<span class="number">0.25</span>,<span class="number">0.375</span>,<span class="number">0.375</span>]]</span><br><span class="line">    <span class="comment">#发射矩阵</span></span><br><span class="line">    emission_probility=[[<span class="number">0.6</span>,<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.05</span>],[<span class="number">0.25</span>,<span class="number">0.25</span>,<span class="number">0.25</span>,<span class="number">0.25</span>],[<span class="number">0.05</span>,<span class="number">0.10</span>,<span class="number">0.35</span>,<span class="number">0.5</span>]]</span><br><span class="line">    <span class="comment">#最后显示状态</span></span><br><span class="line">    obs_seq=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">    <span class="comment">#最后返回一个Row*Col的矩阵结果</span></span><br><span class="line">    F=viterbi(trainsion_probility,emission_probility,pi,obs_seq)</span><br><span class="line">    print(F)</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.cnblogs.com/zhibei/p/9391014.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhibei/p/9391014.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-维特比算法&quot;&gt;&lt;a href=&quot;#1-维特比算法&quot; class=&quot;headerlink&quot; title=&quot;1. 维特比算法&quot;&gt;&lt;/a&gt;1. 维特比算法&lt;/h2&gt;&lt;p&gt;维特比算法是一种&lt;strong&gt;动态规划&lt;/strong&gt;苏算法，可用于最可能产生观测时间序列的
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="维特比算法及其实现" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯简介(Nave Bayes)</title>
    <link href="http://xiuyeliang.com/2020/05/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%80%E4%BB%8B/"/>
    <id>http://xiuyeliang.com/2020/05/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%80%E4%BB%8B/</id>
    <published>2020-05-02T10:11:31.000Z</published>
    <updated>2020-05-06T12:31:53.517Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><h3 id="1-1-条件概率（conditional-probability）"><a href="#1-1-条件概率（conditional-probability）" class="headerlink" title="1.1 条件概率（conditional probability）"></a>1.1 条件概率（conditional probability）</h3><p>$P(A | B)=\frac{P(A B)}{P(B)}$</p><h3 id="1-2-贝叶斯定理"><a href="#1-2-贝叶斯定理" class="headerlink" title="1.2 贝叶斯定理"></a>1.2 贝叶斯定理</h3><p>$P(Y | X)=\frac{P(X | Y) P(Y)}{P(X)}$<br>贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况，我们可以很容易直接得出 $P(X | Y)$ ，$P(Y | X)$则很难得出，但我们更关心 $P(Y | X)$ ，则贝叶斯定理就为我们打通从$P(X | Y)$ 到 $P(Y | X)$ 的道路。</p><h2 id="2-朴素贝叶斯分类"><a href="#2-朴素贝叶斯分类" class="headerlink" title="2. 朴素贝叶斯分类"></a>2. 朴素贝叶斯分类</h2><p>朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。<br>朴素贝叶斯属于<strong>生成模型</strong>：生成式模型由数据学习联合分布 $P(X,Y)$ ,然后求出条件概率分布 $P(Y | X)$  作为预测模型。常见的生成模型有：朴素贝叶斯模型，隐马尔科夫模型，生成对抗网络，变分自动编码器。<br>注：<strong>判别模型</strong>：由判别方法学习到的模型称之为判别模型，判别方法是由数据直接学习决策函数或者条件概率分布 $P(Y | X)$ 作为预测的模型。</p><h3 id="2-1-朴素贝叶斯步骤"><a href="#2-1-朴素贝叶斯步骤" class="headerlink" title="2.1 朴素贝叶斯步骤"></a>2.1 朴素贝叶斯步骤</h3><ol><li>设 $x={a_{1}, a_{2}, \dots, a_{m}}$ 为一个待分类项，而每个a为x的一个特征属性。</li><li>有类别集合 $C={y_{1}, y_{2}, \dots, y_{n}}$ </li><li>计算 $P(y_{1} | x), P(y_{2} | x), \dots, P(y_{n} | x)$ </li><li>如果 $P(y_{k} | x)=\max {P(y_{1} | x), P(y_{2} | x), \dots, P(y_{n} | x)}$ ，则 $x \in y_{k}$。<br>那么关键是如何计算第3步中的各个条件概率。</li><li>找到一个已知分类的待分类项集合，这个集合叫做训练样本集</li><li>统计得到在各类别下各个特征的条件概率估计，即：</li></ol><p>$P\left(a_{1} | y_{1}\right), P\left(a_{2} | y_{1}\right), \dots, P\left(a_{m} | y_{1}\right) ; P\left(a_{1} | y_{2}\right), P\left(a_{2} | y_{2}\right), \dots, P\left(a_{m} | y_{2}\right) ; \dots ; P\left(a_{1} | y_{n}\right), P\left(a_{2} | y_{n}\right), \dots, P\left(a_{m} | y_{n}\right)$</p><ol><li>如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：<br>$P\left(y_{i} | x\right)=\frac{P\left(x | y_{i}\right) P\left(y_{i}\right)}{P(x)}$<br>因为分母对于所有类别为常数，因为我们只要将分子最大化即可，又因为各特征属性是条件独立的，所以有：</li></ol><p>$P\left(x | y_{i}\right) P\left(y_{i}\right)=P\left(a_{1} | y_{i}\right) P\left(a_{2} | y_{i}\right) \dots P\left(a_{m} | y_{i}\right) P\left(y_{i}\right)=P\left(y_{i}\right) \prod_{j=1}^{m} P\left(a_{j} | y_{i}\right)$<br>根据上述分析，朴素贝叶斯分类的流程如下图所示：</p><p><img src="/2020/05/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%80%E4%BB%8B/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="朴素贝叶斯流程图"></p><p>关于 $P(y_{i}), P(a_{j} | y_{i})$ 的求解，有以下三种模型：</p><ol><li><strong>多项式模型</strong>–当特征是离散的时候，使用多项式模型。多项式模型在计算先验概率 $P(y_{i})$和条件概率 $P(a_{j} | y_{i})$ 时，会做一些平滑处理。</li><li><strong>高斯模型</strong>–当特征是连续变量的时候，运用多项式模型就会导致很多 $P(a_{j} | y_{i})=0$ 不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。</li><li><strong>伯努利模型</strong>–与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).<br>伯努利模型中，条件概率 $P(a_{j} | y_{i})$ 的计算方式是：<br>当特征值 $a_i$ 为1时: $P(a_{j} | y_{i})=P(a_{j}=1 | y_{i})$<br>当特征值 $a_i$ 为0时: $P(a_{j} | y_{i})=1-P(a_{j}=1 | y_{i})$</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://blog.csdn.net/guoyunfei20/article/details/78911721" target="_blank" rel="noopener">https://blog.csdn.net/guoyunfei20/article/details/78911721</a></li><li><a href="https://blog.csdn.net/u012162613/article/details/48323777" target="_blank" rel="noopener">https://blog.csdn.net/u012162613/article/details/48323777</a></li><li><a href="https://www.jianshu.com/p/7b673057ab9a" target="_blank" rel="noopener">https://www.jianshu.com/p/7b673057ab9a</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;h3 id=&quot;1-1-条件概率（conditional-probability）&quot;&gt;&lt;a href=&quot;#1-1-条件概率（c
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="朴素贝叶斯" scheme="http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>深入理解语言模型</title>
    <link href="http://xiuyeliang.com/2020/05/02/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>http://xiuyeliang.com/2020/05/02/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-05-02T01:24:05.000Z</published>
    <updated>2020-05-02T07:23:58.813Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-语言模型定义"><a href="#1-语言模型定义" class="headerlink" title="1. 语言模型定义"></a>1. 语言模型定义</h2><p>对于语言序列$(W_1,W_2,W_3,…,W_i) $语言模型就是计算该序列的概率。通俗理解：即判断一个语言序列是否是正常语句，即是否是人话$ P(I Love  U)&gt; P(Love I U)$.</p><h2 id="2-统计语言模型"><a href="#2-统计语言模型" class="headerlink" title="2. 统计语言模型"></a>2. 统计语言模型</h2><h3 id="2-1-n-gram语言模型的基本知识"><a href="#2-1-n-gram语言模型的基本知识" class="headerlink" title="2.1 n-gram语言模型的基本知识"></a>2.1 n-gram语言模型的基本知识</h3><p>首先，由链式法则可以的得到，$ P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,..,w_{n-1})$<br>在统计语言模型中，采用极大似然估计来计算每个词出现的条件概率，即<br>$\begin{aligned} P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right) &amp;=\frac{C\left(w_{1}, w_{2}, \ldots, w_{i}\right)}{\sum_{w} C\left(w_{1}, w_{2}, \ldots w_{i-1}, w\right)} \ &amp; \stackrel{?}{=} \frac{C\left(w_{1}, w_{2}, \ldots, w_{i}\right)}{C\left(w_{1}, w_{2}, \ldots w_{i-1}\right)} \end{aligned}$<br>其中，$C(.)$表示子序列在训练集中出现的次数。但是对于任意长的自然语言语句，根据极大似然估计直接计算$P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right)$显然不现实。<br>为了解决这个问题，引入了<strong>马尔可夫假设（Markov assumption）</strong>,即假设当前词出现的概率只依赖于前n-1个词，可以得到：<br>$P\left(w_{i} | w_{1}, w_{2}, \dots, w_{i-1}\right)=P\left(w_{i} | w_{i-n+1}, \dots, w_{i-1}\right)$</p><p>基于上式，定义<strong>n-gram</strong> 语言模型如下：</p><p>n=1 (unigram) $P\left(w_{1}, w_{2}, \ldots, w_{n}\right)=\prod_{i=1}^{n} P\left(w_{i}\right)$</p><p>n=2 (bigram) $P\left(w_{1}, w_{2}, \dots, w_{n}\right)=\prod_{i=1} P\left(w_{i} | w_{i-1}\right)$</p><p>n=3 (trigram) $P\left(w_{1}, w_{2}, \ldots, w_{n}\right)=\prod_{i=1}^{n} P\left(w_{i} | w_{i-2}, w_{i-1}\right)$</p><p>其中，当n&gt;1时，为了使句首词的条件概率有意思，需要给原序列加上一个或多个启始符。其作用是为了表征句首词出现的条件概率。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>当不加结束符时，n-gram语言模型只能分别对所有固定长度的序列进行概率分布建模，而不是任意长度的序列。</li><li>出现这个现象的原因在于，上述极大似然估计的第二个等式只有在序列包含结束符的时候才成立。</li></ul><h3 id="2-2-n-gram语言中的平滑技术"><a href="#2-2-n-gram语言中的平滑技术" class="headerlink" title="2.2 n-gram语言中的平滑技术"></a>2.2 n-gram语言中的平滑技术</h3><ul><li>Add-one Smoothing(Laplace Smoothing)</li></ul><ul><li><p>Add-K Smoothing</p></li><li><p>Interpolation 核心思路：在计算tri-gram的时候同时考虑Uni-gram,bi-gram,tri-gram出现的频率。</p></li><li><p>Good-Turing Smoothing</p></li></ul><h3 id="2-3-n-gram语言模型小结"><a href="#2-3-n-gram语言模型小结" class="headerlink" title="2.3 n-gram语言模型小结"></a>2.3 n-gram语言模型小结</h3><p>优点：</p><ol><li>采样极大似然估计，参数易训练；</li><li>完全包含了q前n-1个词的全部信息；</li><li>解释性强，直观易理解。<br>缺点</li><li>缺乏啊长期依赖，只能建模到前n-1个词。</li><li>随着n的增大，参数空间呈指数增长</li><li>数据稀疏，难免会出现OOV的问题</li><li>单纯的基于统计频次，泛化能力差。</li></ol><h2 id="3-神经网络语言模型"><a href="#3-神经网络语言模型" class="headerlink" title="3. 神经网络语言模型"></a>3. 神经网络语言模型</h2><p>神经网络语言模型可以看作是在给定一个序列的前提下，预测下一个词出现的概率，$P\left(w_{i} | w_{1}, \dots, w_{i-1}\right)$，不论n-gram中的n怎么取都是对上式的近似。</p><h3 id="3-1-基于前馈神经网络的语言模型"><a href="#3-1-基于前馈神经网络的语言模型" class="headerlink" title="3.1 基于前馈神经网络的语言模型"></a>3.1 基于前馈神经网络的语言模型</h3><p>Bengio[2]在这篇文中提出了如下的前馈神经网络结构（NNLM）。与传统的估计$P\left(w_{i} | w_{1}, \dots, w_{i-1}\right)$不同，NNLM模型直接通过神经网络结构对n元条件概率进行建模，NNLM结构如下。</p><p><img src="/2020/05/02/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/NNLM%E6%A1%86%E6%9E%B6%E5%9B%BE.png" alt="NNLM框架图"></p><h4 id="3-1-1模型输入"><a href="#3-1-1模型输入" class="headerlink" title="3.1.1模型输入"></a>3.1.1模型输入</h4><p>每次从语料库中滑动4个数据形成一个样本，将其中三个词转为one-hot编码形式，将三个one-hot形式作为输入喂入网络。这里用V表示所有单词的集合（即词典），$V_i$表示词典中的第i个单词。</p><h4 id="3-1-2-模型参数"><a href="#3-1-2-模型参数" class="headerlink" title="3.1.2 模型参数"></a>3.1.2 模型参数</h4><p>NNLM的目标是训练如下模型：<br>$f\left(w_{t}, w_{t-1}, \ldots, w_{t-n+2}, w_{t-n+1}\right)=p\left(w_{t} | w_{1}^{t-1}\right)$<br>其中$W_i$ 表示词序列中第t个单词，$w^{t-1}_{1}$表示从第一个词到第t个词组成的子序列。模型需要满足的约束是：</p><ul><li>概率大于0  $f\left(w_{t}, w_{t-1}, \ldots, w_{t-n+2}, w_{t-n+1}\right)&gt;0$</li><li>模型输出的一个向量，该向量的每一个分量依次对应下一个词为词典中某个词的概率。所以｜V｜中一定有一个最大的概率。 $\sum_{i}^{|V|} f\left(w_{t}, w_{t-1}, \dots, w_{t-n+2}, w_{t-n+1}\right)=1$</li></ul><p>模型的传播过程可以分为二部分：特征映射和计算条件概率二部分。</p><ol><li>特征映射： 通过映射矩阵 $C \in R^{|V| \times m}$ 将输入的每一个词映射为一个特征向量，$C(i) \in R^{m}$ 表示词典中第i个词对应的特征向量，其中m表示特征向量的维度。然后将通过特征映射得到 $C\left(w_{t-n+1}\right), \dots, C\left(w_{t-1}\right)$ 合并成一个(n-1)m维的向量 $\left(C\left(w_{t-n+1}\right), \dots, C\left(w_{t-1}\right)\right)$ ，因为每一个词是m维，总共有n-1个词，所以共有(n-1)m维。</li><li>计算条件概率分布：通过一个函数g(g是前馈或者递归神经网络)将输入的词向量序列 $\left(C\left(w_{t-n+1}\right), \dots, C\left(w_{t-1}\right)\right)$ 转化成一个概率分布 $y \in R^{|V|}$ ，这里的输出是|V|维的，和词典的维度是相同的。</li></ol><h4 id="3-1-3-总结"><a href="#3-1-3-总结" class="headerlink" title="3.1.3 总结"></a>3.1.3 总结</h4><p>NNLM模型使用了低维紧凑的词向量对上文进行表示，这解决了词袋模型带来的数据稀疏，语义鸿沟等问题。并且在相似的上下文语境中，NNLM模型可以预测出相似的目标词，而传统模型无法做到这一点。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://mp.weixin.qq.com/s/yQbcQJpFiniZYuulsHVv2Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/yQbcQJpFiniZYuulsHVv2Q</a></li><li><a href="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a></li><li><a href="https://blog.csdn.net/lilong117194/article/details/82018008?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" target="_blank" rel="noopener">https://blog.csdn.net/lilong117194/article/details/82018008?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-语言模型定义&quot;&gt;&lt;a href=&quot;#1-语言模型定义&quot; class=&quot;headerlink&quot; title=&quot;1. 语言模型定义&quot;&gt;&lt;/a&gt;1. 语言模型定义&lt;/h2&gt;&lt;p&gt;对于语言序列$(W_1,W_2,W_3,…,W_i) $语言模型就是计算该序列的概率。通
      
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="深入理解语言模型" scheme="http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>认知心理学-第二章《知觉》概要</title>
    <link href="http://xiuyeliang.com/2020/04/30/%E7%9F%A5%E8%A7%89/"/>
    <id>http://xiuyeliang.com/2020/04/30/%E7%9F%A5%E8%A7%89/</id>
    <published>2020-04-30T05:59:05.000Z</published>
    <updated>2020-04-30T09:23:04.039Z</updated>
    
    <content type="html"><![CDATA[<h2 id="视觉模式识别"><a href="#视觉模式识别" class="headerlink" title="视觉模式识别"></a>视觉模式识别</h2><h3 id="模板匹配模型"><a href="#模板匹配模型" class="headerlink" title="模板匹配模型"></a>模板匹配模型</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模板匹配是通过将刺激（样本特征）与模板进行匹配来识别物体的一种方法。但是一旦对原始样本进行扰动的话，则匹配效果就很差。</span><br></pre></td></tr></table></figure><h3 id="特征分析"><a href="#特征分析" class="headerlink" title="特征分析"></a>特征分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">特征分析首先识别构成模式（样本）的各个特征，然后将其进行组合。</span><br></pre></td></tr></table></figure><h2 id="言语识别"><a href="#言语识别" class="headerlink" title="言语识别"></a>言语识别</h2><h3 id="言语的特征分析"><a href="#言语的特征分析" class="headerlink" title="言语的特征分析"></a>言语的特征分析</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以音素为例，音素识别的依据是音素产生过程中的特征，例如发音部位和浊音音质。</span><br></pre></td></tr></table></figure><h3 id="情景与模式识别"><a href="#情景与模式识别" class="headerlink" title="情景与模式识别"></a>情景与模式识别</h3><p><font color="red">知觉中的一个普遍问题是，这种自上而下加工(情景上下文)与不考虑整体情景而直接对信息本身进行的自下而上加工（样本本身的特征）是如何结合的？</font>比如在字母识别问题中，单词情景可以用来补充特征信息。最后，在马萨罗FLMP模型中指出，情景信息与刺激信息各自独立地提供信息，共同决定知觉到的模式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;视觉模式识别&quot;&gt;&lt;a href=&quot;#视觉模式识别&quot; class=&quot;headerlink&quot; title=&quot;视觉模式识别&quot;&gt;&lt;/a&gt;视觉模式识别&lt;/h2&gt;&lt;h3 id=&quot;模板匹配模型&quot;&gt;&lt;a href=&quot;#模板匹配模型&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
      <category term="书籍" scheme="http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/"/>
    
      <category term="认知心理学" scheme="http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/%E8%AE%A4%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
    
  </entry>
  
</feed>
