{"meta":{"title":"Yeliang Xiu'Blog","subtitle":"","description":"","author":"Yeliang Xiu","url":"http://xiuyeliang.com","root":"/"},"pages":[{"title":"archives","date":"2020-04-25T21:44:54.000Z","updated":"2020-04-25T21:51:27.142Z","comments":true,"path":"archives/index.html","permalink":"http://xiuyeliang.com/archives/index.html","excerpt":"","text":""},{"title":"about","date":"2020-04-25T21:40:53.000Z","updated":"2020-04-25T21:51:45.487Z","comments":true,"path":"about/index.html","permalink":"http://xiuyeliang.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-04-25T21:41:20.000Z","updated":"2020-04-25T21:46:43.032Z","comments":true,"path":"categories/index.html","permalink":"http://xiuyeliang.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-04-25T21:41:09.000Z","updated":"2020-04-25T21:50:59.229Z","comments":true,"path":"tags/index.html","permalink":"http://xiuyeliang.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"深入理解语言模型","slug":"深入理解语言模型","date":"2020-05-02T01:24:05.000Z","updated":"2020-05-02T03:44:01.533Z","comments":true,"path":"2020/05/02/深入理解语言模型/","link":"","permalink":"http://xiuyeliang.com/2020/05/02/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"1. 语言模型定义对于语言序列$(W_1,W_2,W_3,…,W_i) $语言模型就是计算该序列的概率。通俗理解：即判断一个语言序列是否是正常语句，即是否是人话$ P(I Love U)&gt; P(Love I U)$. 2. 统计语言模型2.1 n-gram语言模型的基本知识首先，由链式法则可以的得到，$ P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,..,w_{n-1})$在统计语言模型中，采用极大似然估计来计算每个词出现的条件概率，即$\\begin{aligned} P\\left(w_{i} | w_{1}, \\ldots, w_{i-1}\\right) &amp;=\\frac{C\\left(w_{1}, w_{2}, \\ldots, w_{i}\\right)}{\\sum_{w} C\\left(w_{1}, w_{2}, \\ldots w_{i-1}, w\\right)} \\ &amp; \\stackrel{?}{=} \\frac{C\\left(w_{1}, w_{2}, \\ldots, w_{i}\\right)}{C\\left(w_{1}, w_{2}, \\ldots w_{i-1}\\right)} \\end{aligned}$其中，$C(.)$表示子序列在训练集中出现的次数。但是对于任意长的自然语言语句，根据极大似然估计直接计算$P\\left(w_{i} | w_{1}, \\ldots, w_{i-1}\\right)$显然不现实。为了解决这个问题，引入了马尔可夫假设（Markov assumption）,即假设当前词出现的概率只依赖于前n-1个词，可以得到：$P\\left(w_{i} | w_{1}, w_{2}, \\dots, w_{i-1}\\right)=P\\left(w_{i} | w_{i-n+1}, \\dots, w_{i-1}\\right)$ 基于上式，定义n-gram 语言模型如下： n=1 (unigram) $P\\left(w_{1}, w_{2}, \\ldots, w_{n}\\right)=\\prod_{i=1}^{n} P\\left(w_{i}\\right)$ n=2 (bigram) $P\\left(w_{1}, w_{2}, \\dots, w_{n}\\right)=\\prod_{i=1} P\\left(w_{i} | w_{i-1}\\right)$ n=3 (trigram) $P\\left(w_{1}, w_{2}, \\ldots, w_{n}\\right)=\\prod_{i=1}^{n} P\\left(w_{i} | w_{i-2}, w_{i-1}\\right)$ 其中，当n&gt;1时，为了使句首词的条件概率有意思，需要给原序列加上一个或多个启始符。其作用是为了表征句首词出现的条件概率。 结论 当不加结束符时，n-gram语言模型只能分别对所有固定长度的序列进行概率分布建模，而不是任意长度的序列。 出现这个现象的原因在于，上述极大似然估计的第二个等式只有在序列包含结束符的时候才成立。 2.2 n-gram语言中的平滑技术 Add-one Smoothing(Laplace Smoothing) Add-K Smoothing Interpolation 核心思路：在计算tri-gram的时候同时考虑Uni-gram,bi-gram,tri-gram出现的频率。 Good-Turing Smoothing n-gram语言模型小结优点： 采样极大似然估计，参数易训练； 完全包含了q前n-1个词的全部信息； 解释性强，直观易理解。缺点 缺乏啊长期依赖，只能建模到前n-1个词。 随着n的增大，参数空间呈指数增长 数据稀疏，难免会出现OOV的问题 单纯的基于统计频次，泛化能力差。 神经网络语言模型神经网络语言模型可以看作是在给定一个序列的前提下，预测下一个词出现的概率，$P\\left(w_{i} | w_{1}, \\dots, w_{i-1}\\right)$，不论n-gram中的n怎么取都是对上式的近似。 基于前馈神经网络的语言模型Bengio[2]在这篇文中提出了如下的前馈神经网络结构（NNLM）。与传统的估计$P\\left(w_{i} | w_{1}, \\dots, w_{i-1}\\right)$不同，NNLM模型直接通过神经网络结构对n元条件概率进行建模，NNLM结构如下。 Reference https://mp.weixin.qq.com/s/yQbcQJpFiniZYuulsHVv2Q http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"深入理解语言模型","slug":"自然语言处理/深入理解语言模型","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"}],"tags":[]},{"title":"认知心理学-第二章《知觉》概要","slug":"知觉","date":"2020-04-30T05:59:05.000Z","updated":"2020-04-30T09:23:04.039Z","comments":true,"path":"2020/04/30/知觉/","link":"","permalink":"http://xiuyeliang.com/2020/04/30/%E7%9F%A5%E8%A7%89/","excerpt":"","text":"视觉模式识别模板匹配模型1模板匹配是通过将刺激（样本特征）与模板进行匹配来识别物体的一种方法。但是一旦对原始样本进行扰动的话，则匹配效果就很差。 特征分析1特征分析首先识别构成模式（样本）的各个特征，然后将其进行组合。 言语识别言语的特征分析1以音素为例，音素识别的依据是音素产生过程中的特征，例如发音部位和浊音音质。 情景与模式识别知觉中的一个普遍问题是，这种自上而下加工(情景上下文)与不考虑整体情景而直接对信息本身进行的自下而上加工（样本本身的特征）是如何结合的？比如在字母识别问题中，单词情景可以用来补充特征信息。最后，在马萨罗FLMP模型中指出，情景信息与刺激信息各自独立地提供信息，共同决定知觉到的模式。","categories":[{"name":"书籍","slug":"书籍","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/"},{"name":"认知心理学","slug":"书籍/认知心理学","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/%E8%AE%A4%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%A6/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-04-25T07:12:30.251Z","updated":"2020-04-25T07:12:30.251Z","comments":true,"path":"2020/04/25/hello-world/","link":"","permalink":"http://xiuyeliang.com/2020/04/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"深入理解语言模型","slug":"自然语言处理/深入理解语言模型","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"书籍","slug":"书籍","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/"},{"name":"认知心理学","slug":"书籍/认知心理学","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/%E8%AE%A4%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%A6/"}],"tags":[]}