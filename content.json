{"meta":{"title":"Yeliang Xiu'Blog","subtitle":"","description":"","author":"Yeliang Xiu","url":"http://xiuyeliang.com","root":"/"},"pages":[{"title":"archives","date":"2020-04-25T21:44:54.000Z","updated":"2020-04-25T21:51:27.142Z","comments":true,"path":"archives/index.html","permalink":"http://xiuyeliang.com/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-04-25T21:41:20.000Z","updated":"2020-04-25T21:46:43.032Z","comments":true,"path":"categories/index.html","permalink":"http://xiuyeliang.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-04-25T21:41:09.000Z","updated":"2020-04-25T21:50:59.229Z","comments":true,"path":"tags/index.html","permalink":"http://xiuyeliang.com/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2020-04-25T21:40:53.000Z","updated":"2020-04-25T21:51:45.487Z","comments":true,"path":"about/index.html","permalink":"http://xiuyeliang.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"朴素贝叶斯简介(Nave Bayes)","slug":"朴素贝叶斯简介","date":"2020-05-02T10:11:31.000Z","updated":"2020-05-02T14:01:28.972Z","comments":true,"path":"2020/05/02/朴素贝叶斯简介/","link":"","permalink":"http://xiuyeliang.com/2020/05/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%80%E4%BB%8B/","excerpt":"","text":"1. 前言1.1 条件概率（conditional probability）$P(A | B)=\\frac{P(A B)}{P(B)}$ 1.2 贝叶斯定理$P(B | A)=\\frac{P(A | B) P(B)}{P(A)}$贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况，我们可以很容易直接得出 $P(A | B)$ ，$P(B | A)$则很难得出，但我们更关心 $P(B | A)$ ，则贝叶斯定理就为我们打通从$P(A | B)$ 到 $P(B | A)$ 的道路。 2. 朴素贝叶斯分类朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 2.1 朴素贝叶斯步骤 设 $x=\\left{a_{1}, a_{2}, \\dots, a_{m}\\right}$为一个待分类项，而每个a为x的一个特征属性。 有类别集合 $C=\\left{y_{1}, y_{2}, \\dots, y_{n}\\right}$ 计算 $P\\left(y_{1} | x\\right), P\\left(y_{2} | x\\right), \\dots, P\\left(y_{n} | x\\right)$ 如果 $P\\left(y_{k} | x\\right)=\\max \\left{P\\left(y_{1} | x\\right), P\\left(y_{2} | x\\right), \\dots, P\\left(y_{n} | x\\right)\\right}$ ，则 $x \\in y_{k}$。那么关键是如何计算第3步中的各个条件概率。 找到一个已知分类的待分类项集合，这个集合叫做训练样本集 统计得到在各类别下各个特征的条件概率估计，即： $P\\left(a_{1} | y_{1}\\right), P\\left(a_{2} | y_{1}\\right), \\dots, P\\left(a_{m} | y_{1}\\right) ; P\\left(a_{1} | y_{2}\\right), P\\left(a_{2} | y_{2}\\right), \\dots, P\\left(a_{m} | y_{2}\\right) ; \\dots ; P\\left(a_{1} | y_{n}\\right), P\\left(a_{2} | y_{n}\\right), \\dots, P\\left(a_{m} | y_{n}\\right)$ 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：$P\\left(y_{i} | x\\right)=\\frac{P\\left(x | y_{i}\\right) P\\left(y_{i}\\right)}{P(x)}$因为分母对于所有类别为常数，因为我们只要将分子最大化即可，又因为各特征属性是条件独立的，所以有： $P\\left(x | y_{i}\\right) P\\left(y_{i}\\right)=P\\left(a_{1} | y_{i}\\right) P\\left(a_{2} | y_{i}\\right) \\dots P\\left(a_{m} | y_{i}\\right) P\\left(y_{i}\\right)=P\\left(y_{i}\\right) \\prod_{j=1}^{m} P\\left(a_{j} | y_{i}\\right)$根据上述分析，朴素贝叶斯分类的流程如下图所示： Reference https://blog.csdn.net/guoyunfei20/article/details/78911721","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"朴素贝叶斯","slug":"机器学习/朴素贝叶斯","permalink":"http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"}],"tags":[]},{"title":"深入理解语言模型","slug":"语言模型","date":"2020-05-02T01:24:05.000Z","updated":"2020-05-02T07:23:58.813Z","comments":true,"path":"2020/05/02/语言模型/","link":"","permalink":"http://xiuyeliang.com/2020/05/02/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"1. 语言模型定义对于语言序列$(W_1,W_2,W_3,…,W_i) $语言模型就是计算该序列的概率。通俗理解：即判断一个语言序列是否是正常语句，即是否是人话$ P(I Love U)&gt; P(Love I U)$. 2. 统计语言模型2.1 n-gram语言模型的基本知识首先，由链式法则可以的得到，$ P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,..,w_{n-1})$在统计语言模型中，采用极大似然估计来计算每个词出现的条件概率，即$\\begin{aligned} P\\left(w_{i} | w_{1}, \\ldots, w_{i-1}\\right) &amp;=\\frac{C\\left(w_{1}, w_{2}, \\ldots, w_{i}\\right)}{\\sum_{w} C\\left(w_{1}, w_{2}, \\ldots w_{i-1}, w\\right)} \\ &amp; \\stackrel{?}{=} \\frac{C\\left(w_{1}, w_{2}, \\ldots, w_{i}\\right)}{C\\left(w_{1}, w_{2}, \\ldots w_{i-1}\\right)} \\end{aligned}$其中，$C(.)$表示子序列在训练集中出现的次数。但是对于任意长的自然语言语句，根据极大似然估计直接计算$P\\left(w_{i} | w_{1}, \\ldots, w_{i-1}\\right)$显然不现实。为了解决这个问题，引入了马尔可夫假设（Markov assumption）,即假设当前词出现的概率只依赖于前n-1个词，可以得到：$P\\left(w_{i} | w_{1}, w_{2}, \\dots, w_{i-1}\\right)=P\\left(w_{i} | w_{i-n+1}, \\dots, w_{i-1}\\right)$ 基于上式，定义n-gram 语言模型如下： n=1 (unigram) $P\\left(w_{1}, w_{2}, \\ldots, w_{n}\\right)=\\prod_{i=1}^{n} P\\left(w_{i}\\right)$ n=2 (bigram) $P\\left(w_{1}, w_{2}, \\dots, w_{n}\\right)=\\prod_{i=1} P\\left(w_{i} | w_{i-1}\\right)$ n=3 (trigram) $P\\left(w_{1}, w_{2}, \\ldots, w_{n}\\right)=\\prod_{i=1}^{n} P\\left(w_{i} | w_{i-2}, w_{i-1}\\right)$ 其中，当n&gt;1时，为了使句首词的条件概率有意思，需要给原序列加上一个或多个启始符。其作用是为了表征句首词出现的条件概率。 结论 当不加结束符时，n-gram语言模型只能分别对所有固定长度的序列进行概率分布建模，而不是任意长度的序列。 出现这个现象的原因在于，上述极大似然估计的第二个等式只有在序列包含结束符的时候才成立。 2.2 n-gram语言中的平滑技术 Add-one Smoothing(Laplace Smoothing) Add-K Smoothing Interpolation 核心思路：在计算tri-gram的时候同时考虑Uni-gram,bi-gram,tri-gram出现的频率。 Good-Turing Smoothing 2.3 n-gram语言模型小结优点： 采样极大似然估计，参数易训练； 完全包含了q前n-1个词的全部信息； 解释性强，直观易理解。缺点 缺乏啊长期依赖，只能建模到前n-1个词。 随着n的增大，参数空间呈指数增长 数据稀疏，难免会出现OOV的问题 单纯的基于统计频次，泛化能力差。 3. 神经网络语言模型神经网络语言模型可以看作是在给定一个序列的前提下，预测下一个词出现的概率，$P\\left(w_{i} | w_{1}, \\dots, w_{i-1}\\right)$，不论n-gram中的n怎么取都是对上式的近似。 3.1 基于前馈神经网络的语言模型Bengio[2]在这篇文中提出了如下的前馈神经网络结构（NNLM）。与传统的估计$P\\left(w_{i} | w_{1}, \\dots, w_{i-1}\\right)$不同，NNLM模型直接通过神经网络结构对n元条件概率进行建模，NNLM结构如下。 3.1.1模型输入每次从语料库中滑动4个数据形成一个样本，将其中三个词转为one-hot编码形式，将三个one-hot形式作为输入喂入网络。这里用V表示所有单词的集合（即词典），$V_i$表示词典中的第i个单词。 3.1.2 模型参数NNLM的目标是训练如下模型：$f\\left(w_{t}, w_{t-1}, \\ldots, w_{t-n+2}, w_{t-n+1}\\right)=p\\left(w_{t} | w_{1}^{t-1}\\right)$其中$W_i$ 表示词序列中第t个单词，$w^{t-1}_{1}$表示从第一个词到第t个词组成的子序列。模型需要满足的约束是： 概率大于0 $f\\left(w_{t}, w_{t-1}, \\ldots, w_{t-n+2}, w_{t-n+1}\\right)&gt;0$ 模型输出的一个向量，该向量的每一个分量依次对应下一个词为词典中某个词的概率。所以｜V｜中一定有一个最大的概率。 $\\sum_{i}^{|V|} f\\left(w_{t}, w_{t-1}, \\dots, w_{t-n+2}, w_{t-n+1}\\right)=1$ 模型的传播过程可以分为二部分：特征映射和计算条件概率二部分。 特征映射： 通过映射矩阵 $C \\in R^{|V| \\times m}$ 将输入的每一个词映射为一个特征向量，$C(i) \\in R^{m}$ 表示词典中第i个词对应的特征向量，其中m表示特征向量的维度。然后将通过特征映射得到 $C\\left(w_{t-n+1}\\right), \\dots, C\\left(w_{t-1}\\right)$ 合并成一个(n-1)m维的向量 $\\left(C\\left(w_{t-n+1}\\right), \\dots, C\\left(w_{t-1}\\right)\\right)$ ，因为每一个词是m维，总共有n-1个词，所以共有(n-1)m维。 计算条件概率分布：通过一个函数g(g是前馈或者递归神经网络)将输入的词向量序列 $\\left(C\\left(w_{t-n+1}\\right), \\dots, C\\left(w_{t-1}\\right)\\right)$ 转化成一个概率分布 $y \\in R^{|V|}$ ，这里的输出是|V|维的，和词典的维度是相同的。 3.1.3 总结NNLM模型使用了低维紧凑的词向量对上文进行表示，这解决了词袋模型带来的数据稀疏，语义鸿沟等问题。并且在相似的上下文语境中，NNLM模型可以预测出相似的目标词，而传统模型无法做到这一点。 Reference https://mp.weixin.qq.com/s/yQbcQJpFiniZYuulsHVv2Q http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf https://blog.csdn.net/lilong117194/article/details/82018008?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"深入理解语言模型","slug":"自然语言处理/深入理解语言模型","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"}],"tags":[]},{"title":"认知心理学-第二章《知觉》概要","slug":"知觉","date":"2020-04-30T05:59:05.000Z","updated":"2020-04-30T09:23:04.039Z","comments":true,"path":"2020/04/30/知觉/","link":"","permalink":"http://xiuyeliang.com/2020/04/30/%E7%9F%A5%E8%A7%89/","excerpt":"","text":"视觉模式识别模板匹配模型1模板匹配是通过将刺激（样本特征）与模板进行匹配来识别物体的一种方法。但是一旦对原始样本进行扰动的话，则匹配效果就很差。 特征分析1特征分析首先识别构成模式（样本）的各个特征，然后将其进行组合。 言语识别言语的特征分析1以音素为例，音素识别的依据是音素产生过程中的特征，例如发音部位和浊音音质。 情景与模式识别知觉中的一个普遍问题是，这种自上而下加工(情景上下文)与不考虑整体情景而直接对信息本身进行的自下而上加工（样本本身的特征）是如何结合的？比如在字母识别问题中，单词情景可以用来补充特征信息。最后，在马萨罗FLMP模型中指出，情景信息与刺激信息各自独立地提供信息，共同决定知觉到的模式。","categories":[{"name":"书籍","slug":"书籍","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/"},{"name":"认知心理学","slug":"书籍/认知心理学","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/%E8%AE%A4%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%A6/"}],"tags":[]}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"朴素贝叶斯","slug":"机器学习/朴素贝叶斯","permalink":"http://xiuyeliang.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"深入理解语言模型","slug":"自然语言处理/深入理解语言模型","permalink":"http://xiuyeliang.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"书籍","slug":"书籍","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/"},{"name":"认知心理学","slug":"书籍/认知心理学","permalink":"http://xiuyeliang.com/categories/%E4%B9%A6%E7%B1%8D/%E8%AE%A4%E7%9F%A5%E5%BF%83%E7%90%86%E5%AD%A6/"}],"tags":[]}