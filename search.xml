<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>阅读《A Survey on Knowledge Graphs-Representation, Acquisition and Applications》-AAAI2020</title>
    <url>/2020/05/06/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0ICLR2020/</url>
    <content><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>该综述主要从4个方面对知识图谱相关信息进行总结。</p>
<ol>
<li>知识图谱表示-主要是对图中的实体和关系用低维向量表示。</li>
</ol>
<p><img src="/2020/05/06/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0ICLR2020/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0.png" alt="知识图谱综述"></p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>自然语言处理</category>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title>深入理解语言模型</title>
    <url>/2020/05/02/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="1-语言模型定义"><a href="#1-语言模型定义" class="headerlink" title="1. 语言模型定义"></a>1. 语言模型定义</h2><p>对于语言序列$(W_1,W_2,W_3,…,W_i) $语言模型就是计算该序列的概率。通俗理解：即判断一个语言序列是否是正常语句，即是否是人话$ P(I Love  U)&gt; P(Love I U)$.</p>
<h2 id="2-统计语言模型"><a href="#2-统计语言模型" class="headerlink" title="2. 统计语言模型"></a>2. 统计语言模型</h2><h3 id="2-1-n-gram语言模型的基本知识"><a href="#2-1-n-gram语言模型的基本知识" class="headerlink" title="2.1 n-gram语言模型的基本知识"></a>2.1 n-gram语言模型的基本知识</h3><p>首先，由链式法则可以的得到，$ P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,..,w_{n-1})$<br>在统计语言模型中，采用极大似然估计来计算每个词出现的条件概率，即<br>$\begin{aligned} P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right) &amp;=\frac{C\left(w_{1}, w_{2}, \ldots, w_{i}\right)}{\sum_{w} C\left(w_{1}, w_{2}, \ldots w_{i-1}, w\right)} \ &amp; \stackrel{?}{=} \frac{C\left(w_{1}, w_{2}, \ldots, w_{i}\right)}{C\left(w_{1}, w_{2}, \ldots w_{i-1}\right)} \end{aligned}$<br>其中，$C(.)$表示子序列在训练集中出现的次数。但是对于任意长的自然语言语句，根据极大似然估计直接计算$P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right)$显然不现实。<br>为了解决这个问题，引入了<strong>马尔可夫假设（Markov assumption）</strong>,即假设当前词出现的概率只依赖于前n-1个词，可以得到：<br>$P\left(w_{i} | w_{1}, w_{2}, \dots, w_{i-1}\right)=P\left(w_{i} | w_{i-n+1}, \dots, w_{i-1}\right)$</p>
<p>基于上式，定义<strong>n-gram</strong> 语言模型如下：</p>
<p>n=1 (unigram) $P\left(w_{1}, w_{2}, \ldots, w_{n}\right)=\prod_{i=1}^{n} P\left(w_{i}\right)$</p>
<p>n=2 (bigram) $P\left(w_{1}, w_{2}, \dots, w_{n}\right)=\prod_{i=1} P\left(w_{i} | w_{i-1}\right)$</p>
<p>n=3 (trigram) $P\left(w_{1}, w_{2}, \ldots, w_{n}\right)=\prod_{i=1}^{n} P\left(w_{i} | w_{i-2}, w_{i-1}\right)$</p>
<p>其中，当n&gt;1时，为了使句首词的条件概率有意思，需要给原序列加上一个或多个启始符。其作用是为了表征句首词出现的条件概率。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>当不加结束符时，n-gram语言模型只能分别对所有固定长度的序列进行概率分布建模，而不是任意长度的序列。</li>
<li>出现这个现象的原因在于，上述极大似然估计的第二个等式只有在序列包含结束符的时候才成立。</li>
</ul>
<h3 id="2-2-n-gram语言中的平滑技术"><a href="#2-2-n-gram语言中的平滑技术" class="headerlink" title="2.2 n-gram语言中的平滑技术"></a>2.2 n-gram语言中的平滑技术</h3><ul>
<li>Add-one Smoothing(Laplace Smoothing)</li>
</ul>
<ul>
<li><p>Add-K Smoothing</p>
</li>
<li><p>Interpolation 核心思路：在计算tri-gram的时候同时考虑Uni-gram,bi-gram,tri-gram出现的频率。</p>
</li>
<li><p>Good-Turing Smoothing</p>
</li>
</ul>
<h3 id="2-3-n-gram语言模型小结"><a href="#2-3-n-gram语言模型小结" class="headerlink" title="2.3 n-gram语言模型小结"></a>2.3 n-gram语言模型小结</h3><p>优点：</p>
<ol>
<li>采样极大似然估计，参数易训练；</li>
<li>完全包含了q前n-1个词的全部信息；</li>
<li>解释性强，直观易理解。<br>缺点</li>
<li>缺乏啊长期依赖，只能建模到前n-1个词。</li>
<li>随着n的增大，参数空间呈指数增长</li>
<li>数据稀疏，难免会出现OOV的问题</li>
<li>单纯的基于统计频次，泛化能力差。</li>
</ol>
<h2 id="3-神经网络语言模型"><a href="#3-神经网络语言模型" class="headerlink" title="3. 神经网络语言模型"></a>3. 神经网络语言模型</h2><p>神经网络语言模型可以看作是在给定一个序列的前提下，预测下一个词出现的概率，$P\left(w_{i} | w_{1}, \dots, w_{i-1}\right)$，不论n-gram中的n怎么取都是对上式的近似。</p>
<h3 id="3-1-基于前馈神经网络的语言模型"><a href="#3-1-基于前馈神经网络的语言模型" class="headerlink" title="3.1 基于前馈神经网络的语言模型"></a>3.1 基于前馈神经网络的语言模型</h3><p>Bengio[2]在这篇文中提出了如下的前馈神经网络结构（NNLM）。与传统的估计$P\left(w_{i} | w_{1}, \dots, w_{i-1}\right)$不同，NNLM模型直接通过神经网络结构对n元条件概率进行建模，NNLM结构如下。</p>
<p><img src="/2020/05/02/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/NNLM%E6%A1%86%E6%9E%B6%E5%9B%BE.png" alt="NNLM框架图"></p>
<h4 id="3-1-1模型输入"><a href="#3-1-1模型输入" class="headerlink" title="3.1.1模型输入"></a>3.1.1模型输入</h4><p>每次从语料库中滑动4个数据形成一个样本，将其中三个词转为one-hot编码形式，将三个one-hot形式作为输入喂入网络。这里用V表示所有单词的集合（即词典），$V_i$表示词典中的第i个单词。</p>
<h4 id="3-1-2-模型参数"><a href="#3-1-2-模型参数" class="headerlink" title="3.1.2 模型参数"></a>3.1.2 模型参数</h4><p>NNLM的目标是训练如下模型：<br>$f\left(w_{t}, w_{t-1}, \ldots, w_{t-n+2}, w_{t-n+1}\right)=p\left(w_{t} | w_{1}^{t-1}\right)$<br>其中$W_i$ 表示词序列中第t个单词，$w^{t-1}_{1}$表示从第一个词到第t个词组成的子序列。模型需要满足的约束是：</p>
<ul>
<li>概率大于0  $f\left(w_{t}, w_{t-1}, \ldots, w_{t-n+2}, w_{t-n+1}\right)&gt;0$</li>
<li>模型输出的一个向量，该向量的每一个分量依次对应下一个词为词典中某个词的概率。所以｜V｜中一定有一个最大的概率。 $\sum_{i}^{|V|} f\left(w_{t}, w_{t-1}, \dots, w_{t-n+2}, w_{t-n+1}\right)=1$</li>
</ul>
<p>模型的传播过程可以分为二部分：特征映射和计算条件概率二部分。</p>
<ol>
<li>特征映射： 通过映射矩阵 $C \in R^{|V| \times m}$ 将输入的每一个词映射为一个特征向量，$C(i) \in R^{m}$ 表示词典中第i个词对应的特征向量，其中m表示特征向量的维度。然后将通过特征映射得到 $C\left(w_{t-n+1}\right), \dots, C\left(w_{t-1}\right)$ 合并成一个(n-1)m维的向量 $\left(C\left(w_{t-n+1}\right), \dots, C\left(w_{t-1}\right)\right)$ ，因为每一个词是m维，总共有n-1个词，所以共有(n-1)m维。</li>
<li>计算条件概率分布：通过一个函数g(g是前馈或者递归神经网络)将输入的词向量序列 $\left(C\left(w_{t-n+1}\right), \dots, C\left(w_{t-1}\right)\right)$ 转化成一个概率分布 $y \in R^{|V|}$ ，这里的输出是|V|维的，和词典的维度是相同的。</li>
</ol>
<h4 id="3-1-3-总结"><a href="#3-1-3-总结" class="headerlink" title="3.1.3 总结"></a>3.1.3 总结</h4><p>NNLM模型使用了低维紧凑的词向量对上文进行表示，这解决了词袋模型带来的数据稀疏，语义鸿沟等问题。并且在相似的上下文语境中，NNLM模型可以预测出相似的目标词，而传统模型无法做到这一点。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://mp.weixin.qq.com/s/yQbcQJpFiniZYuulsHVv2Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/yQbcQJpFiniZYuulsHVv2Q</a></li>
<li><a href="http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a></li>
<li><a href="https://blog.csdn.net/lilong117194/article/details/82018008?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1" target="_blank" rel="noopener">https://blog.csdn.net/lilong117194/article/details/82018008?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1</a></li>
</ol>
]]></content>
      <categories>
        <category>自然语言处理</category>
        <category>深入理解语言模型</category>
      </categories>
  </entry>
  <entry>
    <title>认知心理学-第二章《知觉》概要</title>
    <url>/2020/04/30/%E7%9F%A5%E8%A7%89/</url>
    <content><![CDATA[<h2 id="视觉模式识别"><a href="#视觉模式识别" class="headerlink" title="视觉模式识别"></a>视觉模式识别</h2><h3 id="模板匹配模型"><a href="#模板匹配模型" class="headerlink" title="模板匹配模型"></a>模板匹配模型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">模板匹配是通过将刺激（样本特征）与模板进行匹配来识别物体的一种方法。但是一旦对原始样本进行扰动的话，则匹配效果就很差。</span><br></pre></td></tr></table></figure>
<h3 id="特征分析"><a href="#特征分析" class="headerlink" title="特征分析"></a>特征分析</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">特征分析首先识别构成模式（样本）的各个特征，然后将其进行组合。</span><br></pre></td></tr></table></figure>
<h2 id="言语识别"><a href="#言语识别" class="headerlink" title="言语识别"></a>言语识别</h2><h3 id="言语的特征分析"><a href="#言语的特征分析" class="headerlink" title="言语的特征分析"></a>言语的特征分析</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">以音素为例，音素识别的依据是音素产生过程中的特征，例如发音部位和浊音音质。</span><br></pre></td></tr></table></figure>
<h3 id="情景与模式识别"><a href="#情景与模式识别" class="headerlink" title="情景与模式识别"></a>情景与模式识别</h3><p><font color="red">知觉中的一个普遍问题是，这种自上而下加工(情景上下文)与不考虑整体情景而直接对信息本身进行的自下而上加工（样本本身的特征）是如何结合的？</font>比如在字母识别问题中，单词情景可以用来补充特征信息。最后，在马萨罗FLMP模型中指出，情景信息与刺激信息各自独立地提供信息，共同决定知觉到的模式。</p>
]]></content>
      <categories>
        <category>书籍</category>
        <category>认知心理学</category>
      </categories>
  </entry>
  <entry>
    <title>朴素贝叶斯简介(Nave Bayes)</title>
    <url>/2020/05/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><h3 id="1-1-条件概率（conditional-probability）"><a href="#1-1-条件概率（conditional-probability）" class="headerlink" title="1.1 条件概率（conditional probability）"></a>1.1 条件概率（conditional probability）</h3><p>$P(A | B)=\frac{P(A B)}{P(B)}$</p>
<h3 id="1-2-贝叶斯定理"><a href="#1-2-贝叶斯定理" class="headerlink" title="1.2 贝叶斯定理"></a>1.2 贝叶斯定理</h3><p>$P(Y | X)=\frac{P(X | Y) P(Y)}{P(X)}$<br>贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况，我们可以很容易直接得出 $P(X | Y)$ ，$P(Y | X)$则很难得出，但我们更关心 $P(Y | X)$ ，则贝叶斯定理就为我们打通从$P(X | Y)$ 到 $P(Y | X)$ 的道路。</p>
<h2 id="2-朴素贝叶斯分类"><a href="#2-朴素贝叶斯分类" class="headerlink" title="2. 朴素贝叶斯分类"></a>2. 朴素贝叶斯分类</h2><p>朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。<br>朴素贝叶斯属于<strong>生成模型</strong>：生成式模型由数据学习联合分布 $P(X,Y)$ ,然后求出条件概率分布 $P(Y | X)$  作为预测模型。常见的生成模型有：朴素贝叶斯模型，隐马尔科夫模型，生成对抗网络，变分自动编码器。<br>注：<strong>判别模型</strong>：由判别方法学习到的模型称之为判别模型，判别方法是由数据直接学习决策函数或者条件概率分布 $P(Y | X)$ 作为预测的模型。</p>
<h3 id="2-1-朴素贝叶斯步骤"><a href="#2-1-朴素贝叶斯步骤" class="headerlink" title="2.1 朴素贝叶斯步骤"></a>2.1 朴素贝叶斯步骤</h3><ol>
<li>设 $x={a_{1}, a_{2}, \dots, a_{m}}$ 为一个待分类项，而每个a为x的一个特征属性。</li>
<li>有类别集合 $C={y_{1}, y_{2}, \dots, y_{n}}$ </li>
<li>计算 $P(y_{1} | x), P(y_{2} | x), \dots, P(y_{n} | x)$ </li>
<li>如果 $P(y_{k} | x)=\max {P(y_{1} | x), P(y_{2} | x), \dots, P(y_{n} | x)}$ ，则 $x \in y_{k}$。<br>那么关键是如何计算第3步中的各个条件概率。</li>
<li>找到一个已知分类的待分类项集合，这个集合叫做训练样本集</li>
<li>统计得到在各类别下各个特征的条件概率估计，即：</li>
</ol>
<p>$P\left(a_{1} | y_{1}\right), P\left(a_{2} | y_{1}\right), \dots, P\left(a_{m} | y_{1}\right) ; P\left(a_{1} | y_{2}\right), P\left(a_{2} | y_{2}\right), \dots, P\left(a_{m} | y_{2}\right) ; \dots ; P\left(a_{1} | y_{n}\right), P\left(a_{2} | y_{n}\right), \dots, P\left(a_{m} | y_{n}\right)$</p>
<ol>
<li>如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：<br>$P\left(y_{i} | x\right)=\frac{P\left(x | y_{i}\right) P\left(y_{i}\right)}{P(x)}$<br>因为分母对于所有类别为常数，因为我们只要将分子最大化即可，又因为各特征属性是条件独立的，所以有：</li>
</ol>
<p>$P\left(x | y_{i}\right) P\left(y_{i}\right)=P\left(a_{1} | y_{i}\right) P\left(a_{2} | y_{i}\right) \dots P\left(a_{m} | y_{i}\right) P\left(y_{i}\right)=P\left(y_{i}\right) \prod_{j=1}^{m} P\left(a_{j} | y_{i}\right)$<br>根据上述分析，朴素贝叶斯分类的流程如下图所示：</p>
<p><img src="/2020/05/02/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%80%E4%BB%8B/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="朴素贝叶斯流程图"></p>
<p>关于 $P(y_{i}), P(a_{j} | y_{i})$ 的求解，有以下三种模型：</p>
<ol>
<li><strong>多项式模型</strong>–当特征是离散的时候，使用多项式模型。多项式模型在计算先验概率 $P(y_{i})$和条件概率 $P(a_{j} | y_{i})$ 时，会做一些平滑处理。</li>
<li><strong>高斯模型</strong>–当特征是连续变量的时候，运用多项式模型就会导致很多 $P(a_{j} | y_{i})=0$ 不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。</li>
<li><strong>伯努利模型</strong>–与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).<br>伯努利模型中，条件概率 $P(a_{j} | y_{i})$ 的计算方式是：<br>当特征值 $a_i$ 为1时: $P(a_{j} | y_{i})=P(a_{j}=1 | y_{i})$<br>当特征值 $a_i$ 为0时: $P(a_{j} | y_{i})=1-P(a_{j}=1 | y_{i})$</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://blog.csdn.net/guoyunfei20/article/details/78911721" target="_blank" rel="noopener">https://blog.csdn.net/guoyunfei20/article/details/78911721</a></li>
<li><a href="https://blog.csdn.net/u012162613/article/details/48323777" target="_blank" rel="noopener">https://blog.csdn.net/u012162613/article/details/48323777</a></li>
<li><a href="https://www.jianshu.com/p/7b673057ab9a" target="_blank" rel="noopener">https://www.jianshu.com/p/7b673057ab9a</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
  </entry>
  <entry>
    <title>维特比(Viterbi)算法及python实现</title>
    <url>/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="1-维特比算法"><a href="#1-维特比算法" class="headerlink" title="1. 维特比算法"></a>1. 维特比算法</h2><p>维特比算法是一种<strong>动态规划</strong>苏算法，可用于最可能产生观测时间序列的-维特比路径-隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔可夫模型中。</p>
<p>在计算机科学领域中，动态规划的思想解决的最基本的一个问题是：寻找有向图无环图当中二个点之间的最短路径（实际应用于语音识别，词性标注，分词等）</p>
<h2 id="2-例子"><a href="#2-例子" class="headerlink" title="2. 例子"></a>2. 例子</h2><p>如下图所示，求S-&gt;E的最短路径，<br><img src="/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/%E6%88%AA%E5%B1%8F2020-05-03%2016.42.34.png" alt="截屏2020-05-03 16.42.34"></p>
<ol>
<li>最简单的事穷举法，把所有可能的路径都举出来，即$4<em>4</em>4=64$ 种可能。</li>
<li>基于动态规划的方式来寻找最佳路径：思想是把大的问题细分为多个小的问题，基于每一步的结果再去寻找下一步的策略，通过每一步走过之后的局部最优去寻找全局最优，<br>以下是具体做法：<br><img src="/2020/05/03/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E5%AE%9E%E7%8E%B0/%E6%88%AA%E5%B1%8F2020-05-03%2016.43.12.png" alt="截屏2020-05-03 16.43.12"></li>
</ol>
<p><strong>step 1</strong>: 从点S出发，对于第一层的4个节点，算出他们的距离 d(S,A1)，d(S,A2),d(S,A3),d(S,A4),因为只有一步，所以这些距离都是S到他们的最短距离。</p>
<p><strong>step 2</strong>: 对于B层的所有节点(B1,B2,B3,B4),要计算出S到他们的最短距离。我们知道，对于特定的节点B2，从S到它的路径可以经过A层的任何一个节点(A1,A2,A3,A4)。对应的路径长就是d(S,B2)=d(S,Ai)+d(Ai,B2)（其中i=1，2，3，4）。由于A层有4个节点（即i有4个取值），我们要一一计算，然后找到最小值。这样，对于B层的每个节点，都需要进行4次运算，而B层有4个节点，所以共有 $4*4=16$ 次运算。</p>
<p><strong>step 3</strong>:  这一步是该算法的核心。我们从step2计算得出的阶段结果只保留4个最短路径值（每个节点保留一个）。那么，若从B层走向C层来说，该步骤的级数已经不再是 16 ，而是变成4！也就是说，从B层到C层的最短路径只需要基于B层得出的4个结果来计算。这种方法一直持续到最后一个状态，每一步计算的复杂度为相邻两层的计算复杂度为 16的正比！再通俗点说，连接着两两相邻层的计算符合变成了+号，取代了原先的 * 号。用这种方法，只需要进行32 次计算！</p>
<p>上述就是著名的维特比算法，<br>若假设整个网络的宽度为D，网格长度为N，那么穷举法的时间复杂度为 $O(D^N)$ , 而维特比算法的时间复杂度为 $O(ND^2)$.</p>
<h2 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h2><p>以词性标注为例，利用维特比算法对一个句子的词性进行标注。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#第一个参数表示转移概率矩阵 $ P(Z_i|Z_&#123;i-1&#125;)$,第二个参数表示发射概率$P(W_i|Z_i)$,第三个参数表示隐藏变量Z的初始概率 $P(Z_i)$,第四个参数表示要标注的句子对像。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(trainsition_probability,emission_probability,pi,obs_seq)</span>:</span></span><br><span class="line">    <span class="comment">#转换为矩阵进行运算</span></span><br><span class="line">    trainsition_probability=np.array(trainsition_probability)</span><br><span class="line">    emission_probability=np.array(emission_probability)</span><br><span class="line">    pi=np.array(pi)</span><br><span class="line">    obs_seq = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>]  <span class="comment">#句子中的词在词库中的下标位置</span></span><br><span class="line">    <span class="comment"># 最后返回一个Row*Col的矩阵结果</span></span><br><span class="line">    Row = np.array(trainsition_probability).shape[<span class="number">0</span>]<span class="comment">#获取词性的个数</span></span><br><span class="line">    Col = len(obs_seq)<span class="comment">#获取句子中词的个数</span></span><br><span class="line">    <span class="comment">#定义要返回的矩阵，即动态规划中需要维护的矩阵，计算矩阵中的每一个元素值</span></span><br><span class="line">    F=np.zeros((Row,Col))<span class="comment">#行数代表词的个数，列数代表词性的种类</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始状态</span></span><br><span class="line">F[:,<span class="number">0</span>]=pi*np.transpose(emission_probability[:,obs_seq[<span class="number">0</span>]])</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>,Col):<span class="comment">#针对每一列</span></span><br><span class="line">        list_max=[]</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(Row):<span class="comment">#遍历每一行</span></span><br><span class="line">            list_x=list(np.array(F[:,t<span class="number">-1</span>])*np.transpose(trainsition_probability[:,n]))</span><br><span class="line">            <span class="comment">#获取最大概率</span></span><br><span class="line">            list_p=[]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> list_x:</span><br><span class="line">                list_p.append(i*<span class="number">10000</span>)</span><br><span class="line">            list_max.append(max(list_p)/<span class="number">10000</span>)</span><br><span class="line">        F[:,t]=np.array(list_max)*np.transpose(emission_probability[:,obs_seq[t]])</span><br><span class="line">    <span class="keyword">return</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#隐藏状态</span></span><br><span class="line">    invisible=[<span class="string">'Sunny'</span>,<span class="string">'Cloud'</span>,<span class="string">'Rainy'</span>]</span><br><span class="line">    <span class="comment">#初始状态</span></span><br><span class="line">    pi=[<span class="number">0.63</span>,<span class="number">0.17</span>,<span class="number">0.20</span>]</span><br><span class="line">    <span class="comment">#转移矩阵</span></span><br><span class="line">    trainsion_probility=[[<span class="number">0.5</span>,<span class="number">0.375</span>,<span class="number">0.125</span>],[<span class="number">0.25</span>,<span class="number">0.125</span>,<span class="number">0.625</span>],[<span class="number">0.25</span>,<span class="number">0.375</span>,<span class="number">0.375</span>]]</span><br><span class="line">    <span class="comment">#发射矩阵</span></span><br><span class="line">    emission_probility=[[<span class="number">0.6</span>,<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.05</span>],[<span class="number">0.25</span>,<span class="number">0.25</span>,<span class="number">0.25</span>,<span class="number">0.25</span>],[<span class="number">0.05</span>,<span class="number">0.10</span>,<span class="number">0.35</span>,<span class="number">0.5</span>]]</span><br><span class="line">    <span class="comment">#最后显示状态</span></span><br><span class="line">    obs_seq=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">    <span class="comment">#最后返回一个Row*Col的矩阵结果</span></span><br><span class="line">    F=viterbi(trainsion_probility,emission_probility,pi,obs_seq)</span><br><span class="line">    print(F)</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://www.cnblogs.com/zhibei/p/9391014.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhibei/p/9391014.html</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>维特比算法及其实现</category>
      </categories>
  </entry>
  <entry>
    <title>隐马尔可夫模型</title>
    <url>/2020/05/03/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E2%80%98/</url>
    <content><![CDATA[<h2 id="1-马尔科夫过程"><a href="#1-马尔科夫过程" class="headerlink" title="1. 马尔科夫过程"></a>1. 马尔科夫过程</h2><p>在已知的目前状态条件下，它未来的演变不依赖于它以往的演变。马尔科夫过程包括一个初始向量和一个状态转移矩阵。</p>
<h2 id="2-马尔科夫链"><a href="#2-马尔科夫链" class="headerlink" title="2. 马尔科夫链"></a>2. 马尔科夫链</h2><p>时间和状态都是离散的马尔科夫过程称为马尔科夫链。</p>
<h2 id="3-隐马尔可夫模型"><a href="#3-隐马尔可夫模型" class="headerlink" title="3. 隐马尔可夫模型"></a>3. 隐马尔可夫模型</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><p>在某些情况下马尔科夫过程不足以描述我们希望发现的模式。譬如，一个隐居的人可能不能直观的观察到天气的情况，但是有一些海藻。民间的传说告诉我们海藻的状态在某种概率上是和天气的情况相关的。在这种情况下我们有两个状态集合，一个可以观察到的状态集合（海藻的状态）和一个隐藏的状态（天气的状况）。我们希望能找到一个算法可以根据海藻的状况和马尔科夫假设来预测天气的状况。</p>
<p>其中，隐藏状态的数目和可以观察到的状态的数目可能是不一样的。在语音识别中，一个简单的发言也许只需要80个语素来描述，但是一个内部的发音机制可以产生不到80或者超过80种不同的声音。同理，在一个有三种状态的天气系统（sunny、cloudy、rainy）中，也许可以观察到四种潮湿程度的海藻（dry、dryish、damp、soggy）。在此情况下，可以观察到的状态序列和隐藏的状态序列是概率相关的。于是我们可以将这种类型的过程建模为一个隐藏的马尔科夫过程和一个和这个马尔科夫过程概率相关的并且可以观察到的状态集合。</p>
<h3 id="3-2-隐马尔科夫"><a href="#3-2-隐马尔科夫" class="headerlink" title="3.2 隐马尔科夫"></a>3.2 隐马尔科夫</h3><h4 id="3-2-1-隐马尔可夫模型表示"><a href="#3-2-1-隐马尔可夫模型表示" class="headerlink" title="3.2.1 隐马尔可夫模型表示"></a>3.2.1 隐马尔可夫模型表示</h4><p>HMM由隐含状态S,可观测状态O,初始状态概率矩阵pi,隐含状态概率转移矩阵A，可观测转移矩阵B（<strong>发射矩阵</strong>）组成。pi和A决定了状态序列，B决定了观测序列，因此，HMM可以由三元符号表示：<br>$\lambda=(A, B, \pi)$</p>
<h4 id="3-2-2-HMM的二个性质"><a href="#3-2-2-HMM的二个性质" class="headerlink" title="3.2.2 HMM的二个性质"></a>3.2.2 HMM的二个性质</h4><ul>
<li>齐次假设-假设t时刻与t-1之前的所有状态和观测是独立的。<br>$P\left(i_{t} | i_{t-1}, o_{t-1}, i_{t-2}, o_{t-2} \cdots i_{1}, o_{1}\right)=P\left(i_{t} | i_{t-1}\right)$</li>
<li>观测独立性假设-t时刻的观测ot与t时刻之前的所有状态和观测是独立的。<br>$P\left(o_{t} | i_{T}, o_{T}, i_{T-1}, o_{T-1} \cdots i_{1}, o_{1}\right)=P\left(o_{t} | i_{t}\right)$<h4 id="HMM的三个问题"><a href="#HMM的三个问题" class="headerlink" title="HMM的三个问题"></a>HMM的三个问题</h4></li>
</ul>
<ol>
<li>概率计算问题：给定模型 $\lambda=(A, B, \pi)$ 和观测序列 $O={O_1,O_2,O_3 \dots}$ ，计算在模型 $\lambda$ 下观测 $O$ 出现的概率 $P(O|\lambda)$ .（解决算法–&gt;前向-后向算法（动态规划））。</li>
<li>学习问题：已知观测序列 $O={O_1,O_2,O_3 \dots}$ ，估计模型 $\lambda=(A, B, \pi)$ 的参数，使得在该参数下该模型的观测序列 $P(O|\lambda)$ 最大。（解决算法–Baum-Welch算法-EM算法）。</li>
<li>预测问题（解码问题）：已知模型 $\lambda=(A, B, \pi)$ 和观测序列 $O={O_1,O_2,O_3 \dots}$ ，求给定观测序列条件概率 $P(I|O,\lambda)$ 最大的状态序列 $I$ 。</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://www.cnblogs.com/pinking/p/8531405.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinking/p/8531405.html</a></li>
<li><a href="https://blog.csdn.net/z2536083458/article/details/99696875" target="_blank" rel="noopener">https://blog.csdn.net/z2536083458/article/details/99696875</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>隐马尔可夫模型</category>
      </categories>
  </entry>
  <entry>
    <title>逻辑回归模型</title>
    <url>/2020/05/04/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><p>线性回归的表达式： $y(x, w)=w_{0}+w_{1} x_{1}+\ldots+w_{n} x_{n}$</p>
<h2 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2. 逻辑回归"></a>2. 逻辑回归</h2><h3 id="2-1逻辑回归的定义"><a href="#2-1逻辑回归的定义" class="headerlink" title="2.1逻辑回归的定义"></a>2.1逻辑回归的定义</h3><p>线性回归可以预测连续值，但是不能解决分类问题，我们需要根据预测的结果判定其属于正类还是负类。所以逻辑回归就是将线性回归的(−∞,+∞)<br>(−∞,+∞)结果，通过sigmoid函数映射到(0,1)<br>(0,1)之间。<br><img src="/2020/05/04/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E5%9B%BE.png" alt="逻辑回归函数图"></p>
<p><strong>为什么使用sigmoid函数？</strong></p>
<ol>
<li>可以对(−∞,+∞)结果，映射到(0,1)之间，作为概率。</li>
<li>$x&lt;0,$ sigmoid$(x)&lt;\frac{1}{2} ; x&gt;0,$ sigmoid $(x)&gt;\frac{1}{2}$ ，可以将1/2作为决策边界。</li>
<li>数学特性好，求导容易： $g^{\prime}(z)=g(z) \cdot(1-g(z))$</li>
</ol>
<p>线性回归决策函数： $h \theta(x)=\theta^{T} x$<br>将其通过sigmoid函数，获得逻辑回归的决策函数： $h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} x}}$<br>逻辑回归可以写成：<br>$P(y=1 | x ; \theta)=h_{\theta}(x)$<br>$P(y=0 | x ; \theta)=1-h_{\theta}(x)$<br>然后可以写成统一的形式：<br>$p(y | x ; \theta)=\left(h_{\theta}(x)\right)^{y}\left(1-h_{\theta}(x)\right)^{1-y}$</p>
<h3 id="2-2逻辑回归损失函数"><a href="#2-2逻辑回归损失函数" class="headerlink" title="2.2逻辑回归损失函数"></a>2.2逻辑回归损失函数</h3><p>由最大似然估计原理，我们可以通过m个训练样本值，来估计出值，使得似然函数值（所有样本的似然函数之积）最大：<br>$\begin{aligned} L(\theta) &amp;=p(\vec{y} | X ; \theta) \ &amp;=\prod_{i=1}^{m} p\left(y^{(i)} | x^{(i)} ; \theta\right) \ &amp;=\prod_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}} \end{aligned}$<br>求log：<br>$\begin{aligned} \ell(\theta) &amp;=\log L(\theta) \ &amp;=\sum_{i=1}^{m} y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right) \end{aligned}$<br>取负数，得损失函数：<br>$J(\theta)=\frac{1}{m} \sum_{i=1}^{n} \operatorname{cost}\left(h_{\theta}\left(x_{i}\right), y_{i}\right)=-\frac{1}{m}\left[\sum_{i=1}^{n} y_{i} \log h_{\theta}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{\theta}\left(x_{i}\right)\right)\right]$</p>
<p>Question: 为什么逻辑回归采用似然函数，而不是平方损失函数？</p>
<ol>
<li>最小二乘的损失函数是非凸函数。没有全局最优解。</li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://blog.csdn.net/jiaoyangwm/article/details/81139362" target="_blank" rel="noopener">https://blog.csdn.net/jiaoyangwm/article/details/81139362</a></li>
<li><a href="https://blog.csdn.net/ddydavie/article/details/82668141" target="_blank" rel="noopener">https://blog.csdn.net/ddydavie/article/details/82668141</a></li>
<li><a href="https://blog.csdn.net/u012421852/article/details/79575720" target="_blank" rel="noopener">https://blog.csdn.net/u012421852/article/details/79575720</a></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>线性回归与逻辑回归</category>
      </categories>
  </entry>
  <entry>
    <title>HEDGE</title>
    <url>/2020/06/05/HEDGE/</url>
    <content><![CDATA[

	<div class="row">
    <embed src="./HEDGE.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
  </entry>
</search>
